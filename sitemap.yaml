api.md:
  hash: 4512e518bca21bfdbbc97752e007d64f
  summary: 'The API Reference Guide provides a thorough overview of various components
    related to instructors, validation, iteration, and function calls within a programming
    framework. Key topics include OpenAI instructors, DSL validators, iterable structures,
    partial applications, parallel processing, and optional operations through the
    ''maybe'' moniker. It also delves into function call mechanisms, offering developers
    essential information for implementing efficient and robust APIs. This guide serves
    as a vital resource for those seeking to enhance their understanding and application
    of API-related functionalities. Keywords: API reference, instructors, validation,
    iteration, function calls, OpenAI, DSL validators, parallel processing.'
architecture.md:
  hash: db45556b4fe06fce9bfb18b44183c045
  summary: Instructor Architecture is a framework for integrating large language models
    (LLMs) with structured data extraction using Pydantic models. It features core
    components like schema conversion, provider-specific adapters, response parsing,
    validation, and retry mechanisms to ensure consistent and accurate data extraction
    across various LLM providers. The system streamlines request formatting, response
    handling, and validation, enabling seamless and reliable interaction with different
    LLM APIs for structured output generation. Key concepts include LLM integration,
    structured data extraction, Pydantic validation, multi-provider support, and automated
    retries for validation failures.
blog/index.md:
  hash: 04ec2689ed366f014bc3f15ce4fd0b42
  summary: This document outlines various resources and updates available for users
    interested in AI development, optimization, and language model techniques. It
    encourages subscribing to a newsletter to receive updates on new features and
    tips for using "Instructor." The content includes topics on advanced AI techniques
    like the Unified Provider Interface, llms.txt adoption, and GPT-4 level summaries
    using GPT-3.5-turbo. It also covers AI model validation, function caching in Python,
    batch processing, and integrations with tools like Logfire and Pandas. Additionally,
    it introduces prompting techniques such as Least-to-Most prompting and the Reverse
    Chain of Thought (RCoT) for enhancing language model performance. Key objectives
    are to keep users informed with the latest advancements and provide practical
    tips for AI model refinement and deployment. Keywords include AI development,
    language models, optimization, Python, integrations, and prompting techniques.
blog/posts/aisummit-2023.md:
  hash: 7c81d54958376008660fb442f25d9c35
  summary: "The blog post highlights a keynote speech delivered at the inaugural AI\
    \ Engineer Summit, focusing on the use of Pydantic for effective prompt engineering\
    \ in machine learning and AI applications. The talk, titled \"Pydantic is all\
    \ you need,\" explores insights beyond typical documentation, emphasizing Pydantic\u2019\
    s role in data validation and its potential to enhance AI performance. Readers\
    \ are encouraged to view the full presentation on YouTube to gain a deeper understanding\
    \ and provide feedback, contributing to the continuous improvement of prompt engineering\
    \ techniques. Key topics include Pydantic, prompt engineering, AI Summit, and\
    \ machine learning."
blog/posts/announcing-gemini-tool-calling-support.md:
  hash: 9918d92d63a5005bc11f4df8593d1411
  summary: "This article introduces the latest support for structured outputs via\
    \ tool calling in the instructor library for both Gemini and VertexAI SDKs, enhancing\
    \ AI model interactions. It highlights easy installation options for Gemini (`instructor[google-generativeai]`)\
    \ and VertexAI (`instructor[vertexai]`), emphasizing Gemini\u2019s advantages\
    \ such as a higher free token quota and simpler setup with just a Google API key.\
    \ The guide provides step-by-step examples of using instructor with Gemini and\
    \ VertexAI models (`gemini-1.5-flash-latest`, `gemini-1.5-pro-latest`) for chat\
    \ completions and structured output extraction, focusing on AI SDKs, tool calling,\
    \ structured outputs, and generative models for AI developers."
blog/posts/announcing-instructor-responses-support.md:
  hash: 8ce4314b2dee3e0af9a37baeee08ed87
  summary: The announcement highlights Instructor's integration with OpenAI's new
    Responses API, providing a streamlined, type-safe interface for structured outputs,
    web search, and citation tools. Key features include easy client initialization,
    full Pydantic validation, built-in tools for real-time information retrieval,
    and async support. This integration enhances LLM applications by simplifying external
    data referencing, maintaining compatibility with existing chat workflows, and
    enabling powerful capabilities like file search and citations without additional
    complexity. Core keywords include Instructor, Responses API, OpenAI, structured
    outputs, type safety, web search, citations, Pydantic, async support, LLM development.
blog/posts/announcing-unified-provider-interface.md:
  hash: 0efef4167acf4ff65db6f455fb32f327
  summary: The article introduces the `from_provider()` function in Instructor, designed
    to simplify the initialization of LLM clients across various providers using a
    single string identifier. This function automates the setup by detecting the provider,
    importing necessary SDKs, and applying patches for structured outputs, making
    it easier to experiment with models from OpenAI, Anthropic, Google, and others.
    Key benefits include streamlined model comparison, multi-provider strategies,
    and rapid prototyping, all while supporting async processing for efficiency. The
    feature aims to enhance workflows by reducing repetitive code, facilitating quick
    provider switches, and offering auto-complete support for ease of use. It is particularly
    valuable for developers looking to optimize LLM performance, cost, and output
    quality across different platforms.
blog/posts/anthropic-prompt-caching.md:
  hash: 3d7128d360a8d981dd809f1fb2790f9b
  summary: This article explains the benefits of using prompt caching with Anthropic's
    API to improve response times and reduce costs in large context applications.
    It introduces the beta prompt caching feature, highlights its limitations such
    as minimum cache size and tool definition restrictions, and demonstrates how to
    implement caching effectively. A detailed example shows how caching a large text
    (e.g., Jane Austen's "Pride and Prejudice") can optimize character extraction
    tasks by avoiding repeated token processing, resulting in faster responses and
    lower expenses. Key topics include prompt caching, Anthropic API optimization,
    cost reduction, latency improvements, token management, and best practices for
    large-context AI workflows.
blog/posts/anthropic-web-search-structured.md:
  hash: 9a5a79e8e389eb7265944a8968db3fa9
  summary: Learn how to leverage Anthropic's web search tool with Instructor to access
    real-time, structured data from the web. This powerful combination enables AI
    models like Claude to fetch the latest information, generate organized responses
    using Pydantic models, and cite sources for verification. Key features include
    enhanced accuracy, reduced hallucinations, and customizable search configurations
    like domain restrictions and search limits. Ideal for building dynamic applications
    that require up-to-date data on topics such as sports, news, or market trends.
blog/posts/anthropic.md:
  hash: 44073f09c95cb56e33653923ef4e83c8
  summary: This article discusses integrating Anthropic's powerful language models
    with Instructor and Pydantic for structured output generation in Python. It provides
    step-by-step guidance on installing the `instructor[anthropic]` package, configuring
    the Anthropic client with enhanced capabilities, and creating custom data models
    for precise JSON responses. Key topics include handling nested types, leveraging
    the `anthropic` client, and supporting models like Claude-3 for AI-driven applications.
    The content highlights ongoing feature development, including streaming support,
    and encourages community feedback to improve compatibility and functionality in
    API development and LLM techniques.
blog/posts/bad-schemas-could-break-llms.md:
  hash: 8d3274500a88eb0bfe0171d9f00504f8
  summary: This article emphasizes the critical impact of response models and schemas
    on Large Language Model (LLM) performance, particularly with Claude and GPT-4o.
    Key insights include how field naming, chain-of-thought reasoning, and response
    mode choices (JSON vs. Tool Calling) significantly influence accuracy, with performance
    gains of up to 60% through optimized schemas. The content highlights the importance
    of designing well-structured response models, testing different permutations systematically,
    and using tools like Instructor for prototyping. Core keywords include LLM response
    models, structured outputs, JSON mode, tool calling, GPT-4o, Claude, reasoning
    prompts, and model performance optimization.
blog/posts/best_framework.md:
  hash: 41b529a5e2d92400da24c6f6c1e8146f
  summary: Instructor is a lightweight Python library that enhances the OpenAI SDK
    by enabling seamless mapping of LLM outputs to structured, type-safe data using
    Pydantic models and Python type annotations. It simplifies extracting structured
    data from GPTs and other compatible providers, supports features like retrying,
    validation, streaming, and parallel tool calling, and allows direct access to
    message parameters for advanced prompt engineering. Designed for easy integration
    and incremental adoption, Instructor helps teams convert unstructured LLM text
    into validated data, making it ideal for improving data consistency and reducing
    "string hell" in AI applications. Key keywords include LLM outputs, structured
    data, Python, Pydantic, OpenAI SDK, GPT, data mapping, response_model.
blog/posts/caching.md:
  hash: 11fdb88f500185d84f0a06cc2a4b4c41
  summary: This article explores advanced caching techniques in Python to optimize
    performance when working with Pydantic models and language model APIs like OpenAI.
    It covers in-memory caching with `functools.cache`, persistent caching with `diskcache`,
    and distributed caching using `redis`. The content emphasizes creating custom
    decorators to cache API responses effectively, with a focus on serialization,
    cache invalidation considerations, and selecting appropriate caching strategies
    for small and large-scale applications. Keywords include Python caching, Pydantic
    models, performance optimization, in-memory caching, diskcache, Redis, API response
    caching, and distributed systems.
blog/posts/chain-of-density.md:
  hash: 1ff99278946f900cba0eb4b22d8c663a
  summary: "This article explores advanced AI summarization techniques, focusing on\
    \ the Chain of Density method with GPT-3.5 and GPT-4. It details how to implement\
    \ iterative, entity-dense summaries, fine-tune GPT-3.5 models for improved performance,\
    \ and achieve significant efficiency gains\u2014up to 20x faster and 50x cost\
    \ savings. The guide covers data modeling, validation with Pydantic, and custom\
    \ prompting for high-quality summaries. Keywords include GPT-3.5, GPT-4, Chain\
    \ of Density, summarization, fine-tuning, LLM techniques, entity density, AI text\
    \ summarization, Instructor library, model distillation, OpenAI, cost efficiency,\
    \ latency reduction."
blog/posts/chat-with-your-pdf-with-gemini.md:
  hash: a8a671863e1994b8a0902f3cac95325c
  summary: "This article demonstrates how to efficiently process PDFs using Google's\
    \ Gemini 1.5 model combined with Instructor for structured data extraction. It\
    \ highlights the challenges of traditional PDF parsing methods and introduces\
    \ Gemini\u2019s multimodal capabilities for handling various file types, including\
    \ long documents, videos, and audio. The guide provides a step-by-step setup using\
    \ Python, showing how to upload a PDF, wait for processing, and obtain structured\
    \ summaries with Pydantic validation. Key features include simple integration,\
    \ reliable structured outputs, and multimodal support, making document processing\
    \ faster, easier, and less complex with large language models. Keywords: Gemini,\
    \ Instructor, PDF processing, structured data extraction, multimodal AI, Python,\
    \ LLMs, document analysis."
blog/posts/citations.md:
  hash: 6231da28195dd069a07ada649be67aad
  summary: This article explores how Pydantic enhances data accuracy and reliability
    in language model applications through citation verification. Using simple substring
    checks, LLM-powered validation, and alignment techniques, the post demonstrates
    methods for ensuring that responses are correctly backed by context. Key concepts
    include Pydantic validators, remote LLM verification with OpenAI, and techniques
    for generating accurate datasets. Keywords include Pydantic, citation verification,
    language models (LLM), data accuracy, Pytyndict, validation, OpenAI, and Python.
blog/posts/consistent-stories.md:
  hash: b11eb15649a2a818d4d6bfcf26507cdb
  summary: 'This article discusses how to generate complex Directed Acyclic Graphs
    (DAGs) using GPT-4o, focusing on creating consistent and coherent Choose Your
    Own Adventure stories. The challenge of generating large graphs is addressed with
    a two-phase approach: first generating a story outline, then expanding choices
    in parallel to manage context limitations and allow deeper story branches. Key
    benefits include path-specific context, parallel generation, controlled growth
    via a max_depth parameter, and rate-limiting using semaphores. The article emphasizes
    structured validation, using Pydantic models, and highlights the efficiency of
    parallel processing for content generation in large-scale language models, applicable
    through tools like instructor with OpenAI''s API. Keywords: DAGs, GPT-4o, Choose
    Your Own Adventure, story generation, language models, parallel processing, Pydantic,
    OpenAI.'
blog/posts/course.md:
  hash: 8424fc0d6b49b24ad11707b30daaddde
  summary: 'Discover a free, one-hour course on Weights and Biases, exploring essential
    techniques for steering language models in machine learning. This comprehensive
    course covers material from detailed tutorials and is accessible to everyone interested
    in AI and machine learning. Perfect for both beginners and experienced practitioners,
    it offers valuable insights and practical tools for leveraging language models
    effectively. Access this open resource at [wandb.courses](https://www.wandb.courses/courses/steering-language-models).
    Keywords: Weights and Biases, language models, machine learning, AI course, free
    resources.'
blog/posts/cursor-rules.md:
  hash: ab556c760792c36dbf7ae8afaa577d36
  summary: The article "Instructor Adopting Cursor Rules" discusses enhancing Git
    workflows for AI-assisted coding through Cursor rules, particularly in the context
    of the Instructor project. It addresses the common issue of disorganized commits
    that arise from "vibe coding," where developers leverage AI like Cursor to quickly
    generate code. The rules, stored in `.cursor/rules` as markdown files, guide contributors
    to make well-structured branches, small and frequent commits, and properly formatted
    PR descriptions. Key benefits include simplified pull requests (PRs), updated
    documentation, and the use of stacked PRs for incremental development of complex
    features. The article encourages developers to contribute to Instructor by following
    these practices, ensuring clarity, teamwork, and an organized commit history while
    allowing AI to handle complex Git tasks. It also invites others to implement similar
    rules in their projects for improved Git practices with AI coding assistance.
blog/posts/distilation-part1.md:
  hash: 2b0cffc5cf2701d20f0f294b843aaf1e
  summary: This guide explores using the `Instructor` library to enhance Python functions
    through fine-tuning and distillation. The library streamlines the process of developing
    task-specific language models by simplifying function calls and managing data
    preparation. Key features include automatic dataset generation for fine-tuning,
    efficient function integration, and backward compatibility. The guide covers logging
    outputs, the importance of structured outputs, and future plans for function implementation.
    Essential keywords include Instructor, fine-tuning, distillation, language models,
    Python, and dataset generation.
blog/posts/extract-model-looks.md:
  hash: 1a96f01876050a880e6d2f67bee23cb2
  summary: "This article presents a two-phase, parallel approach to generating complex,\
    \ consistent Directed Acyclic Graphs (DAGs) and stories with GPT-4o, overcoming\
    \ limitations of large graph sizes and context window constraints. By first creating\
    \ a detailed story outline\u2014including setting, plot, choices, and visual style\u2014\
    and then expanding branches concurrently while maintaining path-specific context,\
    \ the method ensures coherence and efficiency. Key concepts include state isolation,\
    \ parallel processing, structured validation with Pydantic, and controllable story\
    \ depth. Ideal for generating large, interconnected content at scale, this approach\
    \ enhances story and graph generation speed, consistency, and complexity using\
    \ AI models like OpenAI\u2019s GPT-4o."
blog/posts/extracting-model-metadata.md:
  hash: 20e174aa6fc812ccdb8348f6609cf725
  summary: "This article explores how multimodal language models like GPT-4o and the\
    \ instructor library enable structured metadata extraction from images for e-commerce\
    \ and fashion applications. It demonstrates mapping images to a product taxonomy\u2014\
    covering categories, subcategories, product types, and colors\u2014using YAML\
    \ configurations and response validation. The workflow includes loading images\
    \ with instructor\u2019s Image class, defining response models with validation,\
    \ and performing API calls to generate structured metadata. This approach facilitates\
    \ personal style analysis, product recommendations, and enhanced personalization\
    \ by bridging unstructured visual data with structured product information. Keywords\
    \ include multimodal models, structured metadata, image analysis, product taxonomy,\
    \ e-commerce, fashion, instructor library, GPT-4o, and personalization."
blog/posts/fake-data.md:
  hash: aaf0cc8eb89097c0827aa85b6016f009
  summary: "The article provides a guide on generating synthetic data using Pydantic\
    \ and OpenAI's models, focusing specifically on practical examples and configurations\
    \ with Python. It demonstrates how to use Pydantic models and the instructor library\
    \ to interact with OpenAI's GPT models for creating synthetic user data. Key techniques\
    \ include leveraging model and field-level examples, using complex structures\
    \ like JSON schema, and adjusting field descriptions to influence the nature of\
    \ generated data\u2014such as creating names with certain cultural characteristics.\
    \ Core keywords and themes include Synthetic Data, Pydantic, OpenAI, Data Generation,\
    \ and Python. The article also highlights how these methods can be refined by\
    \ switching between GPT-3.5-turbo and gpt-4-turbo-preview models to achieve more\
    \ nuanced outputs."
blog/posts/full-fastapi-visibility.md:
  hash: b86decf8772b03d62dd49c2700936cc3
  summary: This article demonstrates how Logfire enhances FastAPI applications with
    comprehensive observability and OpenTelemetry integration. It highlights easy
    setup and code integration for logging, profiling, and monitoring API endpoints,
    including handling asynchronous operations with asyncio and streaming responses
    using Instructor's Iterable support. Key topics include FastAPI, Logfire, OpenTelemetry,
    Pydantic, AsyncIO, streaming responses, and performance tracking, providing practical
    examples to improve application visibility, debugging, and error reproduction
    in production environments.
blog/posts/generating-pdf-citations.md:
  hash: d293a327202394d87adcd15ec894381e
  summary: This article demonstrates how to leverage Google's Gemini model with Instructor
    and Pydantic for accurate PDF data extraction and citation generation. It highlights
    the importance of structured outputs to reduce hallucinations, ensure source-truthfulness,
    and improve reliability in document processing. The process involves PDF parsing
    with PyMuPDF, uploading files to Gemini, and creating citations for precise referencing,
    making it ideal for legal, academic, and financial applications. Key topics include
    PDF analysis, structured data validation, GPT integration, citation highlighting,
    and reducing errors in AI-generated content, with keywords like Gemini, PDF processing,
    citations, structured outputs, Pydantic, document verification, and AI accuracy.
blog/posts/generator.md:
  hash: b9ebcb6883c21f0ba7d87980c45817dd
  summary: 'This article explores the use of Python generators to enhance Large Language
    Model (LLM) streaming, improving latency and user experience in applications like
    eCommerce and chat interfaces. It explains how generators enable efficient, real-time
    data processing and extraction, allowing for faster rendering and responsiveness.
    The post demonstrates practical implementations using the Instructor library for
    structured data extraction from streaming LLM responses, highlighting their benefits
    over traditional approaches. Key concepts include Python generators, LLM streaming,
    data pipeline optimization, and fast API integration, emphasizing how real-time
    streaming can boost performance and customer engagement. Core keywords: Python
    generators, LLM streaming, data processing, real-time API, latency reduction,
    fastapi, instructor library, structured extraction, performance optimization.'
blog/posts/google-openai-client.md:
  hash: 26e8561156b73b2a9b6da501c1aa7c04
  summary: "This article explains why Instructor remains essential despite Google's\
    \ recent OpenAI compatibility for Gemini models. While the new integration simplifies\
    \ interactions with Gemini via OpenAI's API, it has limitations such as limited\
    \ schema support, lack of streaming, and no multimodal capabilities. Instructor\
    \ offers a provider-agnostic API, advanced schema management, streaming, multimodal\
    \ support, automatic validation, retries, and seamless provider switching\u2014\
    features crucial for building reliable, production-grade LLM applications. Keywords\
    \ include Gemini, OpenAI integration, Instructor, multimodal support, schema management,\
    \ streaming, provider agnostic, robust AI applications."
blog/posts/introducing-structured-outputs-with-cerebras-inference.md:
  hash: 9cae7568e3f7431ca1ee3b73b8a7a1b0
  summary: Explore how to leverage Cerebras Inference for structured outputs and faster
    model processing with seamless Pydantic integration. Cerebras offers up to 20x
    faster inference compared to GPUs, making it an excellent choice for efficient
    API development. The article guides you through setting up a Cerebras Inference
    API key and using the Cerebras SDK with Pydantic models for validated responses.
    Key functionality includes creating instructor clients, using models like "llama3.1-70b",
    and supporting both synchronous and asynchronous operations. Enhance your API
    integration with features such as streaming responses in `CEREBRAS_JSON` mode
    for real-time data processing. Key topics include Cerebras Inference, Pydantic,
    fast inference, structured outputs, and API integration.
blog/posts/introducing-structured-outputs.md:
  hash: 52422024e0e8154c428411dc32f6c13a
  summary: 'This article evaluates OpenAI''s Structured Outputs, highlighting its
    limitations such as limited validation, streaming challenges, and unpredictable
    latency spikes. It introduces `instructor` as a robust alternative that offers
    automatic validation with retries, real-time streaming validation, and a provider-agnostic
    API for seamless switching between OpenAI, Anthropic, and other models. `Instructor`
    addresses key challenges in building reliable, flexible, and efficient LLM workflows,
    making it an ideal solution for developers seeking enhanced data validation, error
    correction, and multi-provider support in AI applications. Key words: Structured
    Outputs, instructor, OpenAI, validation, streaming, latency, multi-provider, JSON
    schema, data validation, LLM workflow.'
blog/posts/introduction.md:
  hash: 33cd1df34b63e686b253b5ebca7b433d
  summary: This article explores how Pydantic simplifies working with Language Learning
    Models (LLMs) in Python, particularly through structured JSON outputs. It highlights
    the difficulties developers face with existing LLM frameworks and showcases how
    the Pydantic-powered Instructor library streamlines interactions with language
    models, focusing on ease of use, widespread adoption, and compatibility with tools
    like OpenAI's Function Calling. By supporting modular schemas, easy validation,
    and relationship definition, Pydantic offers a more organized code structure,
    enhancing the developer experience. The piece also parallels LLM architecture
    with FastAPI, offering simple, Pythonic approaches to utilizing LLMs effectively.
    Key phrases include Pydantic, LLMs, structured JSON, OpenAI, Python, and language
    model interaction.
blog/posts/jinja-proposal.md:
  hash: 729b4faf0f966c13b2b5dfe4c4df96e6
  summary: The article discusses integrating Jinja templating into the Instructor
    platform to enhance formatting, validation, versioning, and secure logging. By
    incorporating Jinja, the platform can better handle complex prompt formatting,
    separate structure and content, and manage dynamic elements through context dictionaries.
    The integration also leverages Pydantic for data validation, enabling features
    like sensitive information redaction and secure logging. This approach improves
    the robustness of Instructor by allowing better version control, structured logging,
    and adherence to software design best practices. Key concepts include Jinja templating,
    Pydantic models, data validation, API development, and secure data handling.
blog/posts/langsmith.md:
  hash: 3f9c1608a2030bf77928eb024d6326e4
  summary: "This blog post explores how LangChain's LangSmith can be integrated with\
    \ the OpenAI client to enhance functionality through seamless LLM observability.\
    \ By wrapping the OpenAI client with LangSmith and using the `instructor` package,\
    \ developers can improve their LLM applications by enabling features such as question\
    \ classification and asynchronous processing with `asyncio`. The article provides\
    \ a step-by-step guide on setting up LangSmith, installing necessary SDKs, and\
    \ implementing multi-label classification of questions using Python. It highlights\
    \ LangSmith\u2019s capabilities as a DevOps platform for developing, collaborating,\
    \ deploying, and monitoring language model applications. Key points include the\
    \ use of `wrap_openai`, rate limiting via `asyncio.Semaphore`, and customizing\
    \ the classification prompt to fit specific use cases."
blog/posts/learn-async.md:
  hash: 1797f121d315159e09da299300d8d5bf
  summary: 'This article explores efficient async processing in Python using `asyncio`,
    OpenAI''s `AsyncOpenAI`, and Instructor for batch data extraction and analysis.
    It highlights key techniques such as using `asyncio.gather`, `asyncio.as_completed`,
    and semaphores for rate limiting to optimize concurrent requests. The guide compares
    different methods'' execution speeds, emphasizing the importance of balancing
    speed, resource management, and server considerations. Core concepts include async/await,
    concurrency, rate limiting, and practical applications in web development and
    data processing. Keywords: asyncio, OpenAI, async programming, data extraction,
    concurrent processing, rate limiting, Python, `gather`, `as_completed`.'
blog/posts/llm-as-reranker.md:
  hash: b4c3b60a958d5cf06f9db76a53b2fc0d
  summary: 'Learn how to build an LLM-based reranker for retrieval-augmented generation
    (RAG) pipelines using Instructor and Pydantic. This tutorial demonstrates how
    to define structured output models, create a reranking function leveraging OpenAI''s
    GPT-4o-mini model, and evaluate search result relevance through chain-of-thought
    reasoning. Key concepts include structuring labels with relevancy scores, sorting
    reranked results, and ensuring data validation with Pydantic validators. This
    approach enhances search accuracy by intelligently reordering results, making
    it ideal for improving search relevance, reranking, evaluation pipelines, and
    fine-tuning traditional rerankers. Keywords: LLM, Pydantic, Instructor, search
    reranking, RAG, relevance scoring, GPT-4, structured outputs.'
blog/posts/llms-txt-adoption.md:
  hash: 6854f91ca9cc4558725ce3687858c19e
  summary: Instructor has adopted the llms.txt specification, designed to make documentation
    more accessible to AI language models. Developed by Jeremy Howard and the Answer.AI
    team, llms.txt functions like robots.txt but for AI, providing a standardized
    way for AI systems to interpret and access documentation efficiently. This implementation
    allows AI coding assistants such as GitHub Copilot to deliver more accurate code
    suggestions by accessing clean markdown versions of documentation. Core benefits
    include enhanced AI integration, cleaner documentation access, and support for
    a growing AI-documentation standard. Instructor's llms.txt file offers an overview,
    key documentation links, and best practices, facilitating superior AI-assisted
    coding experiences and improved understanding of the library's features.
blog/posts/logfire.md:
  hash: 7ce79e21910ace0347fba9fd9615cfca
  summary: The article introduces **Logfire**, an observability platform developed
    by the creators of **Pydantic**, which integrates seamlessly with libraries like
    **HTTPx** and **Instructor**. It demonstrates how Logfire can enhance application
    performance tracking through examples such as spam email classification, validation
    using `llm_validator`, and data extraction from images with **GPT-4V**. The guide
    details how to set up and use these features with Logfire, emphasizing its ease
    of integration, efficient logging capabilities, and ability to provide in-depth
    insights into application processes. Core components include **OpenAI**, **Logfire**,
    **LLM Observability**, and integration with Pydantic.
blog/posts/matching-language.md:
  hash: 4c9a2ae7f458fcaf1d495e3c719f7099
  summary: This post explores methods to ensure language models generate summaries
    that match the source text's language. The issue arises when models tend to produce
    summaries in English, even for non-English texts, due to English-based instructions.
    The article demonstrates using Pydantic for data validation and the `langdetect`
    library for language detection. It highlights an experiment with the GPT-3.5-turbo
    model, showing how updating a model to include a "detected_language" field significantly
    improved accuracy in maintaining the source language. Key topics include multilingual
    summarization, addressing biases in language models, and leveraging technology
    for accurate language detection, making it vital for improving language processing
    tasks and enhancing communication through accurate translations.
blog/posts/migrating-to-uv.md:
  hash: 226ee4a165a8d84023029357089b8443
  summary: This article details the migration from Poetry to UV for dependency management
    and build automation in a Python project. The author highlights UV's faster CI/CD
    performance, automatic caching, cargo-style lockfiles, and easier adoption of
    new PEP features. The article provides a step-by-step guide to converting Poetry
    lockfiles using UV, updating build configurations to use hatchling, and modifying
    GitHub Actions workflows to implement UV commands like `uv sync` and `uv run`.
    Overall, the transition resulted in a ~3x speed increase in CI jobs, simplifying
    dependency management and enhancing development efficiency. Keywords include UV,
    Poetry migration, dependency management, CI/CD speedup, Python, build automation,
    UV lockfile, GitHub actions.
blog/posts/multimodal-gemini.md:
  hash: 7895ceb43c6914bb9d3102c9703f09f7
  summary: This article explores how to leverage Google's Gemini multimodal AI model
    with Instructor for structured extraction from travel videos. It demonstrates
    processing YouTube videos to generate detailed, structured recommendations for
    tourist destinations using Pydantic models in Python. Key features include analyzing
    video content to identify places, markets, cafes, and heritage sites with precise
    details, enabling enhanced travel planning. The post also discusses current limitations
    like lack of timestamping and speaker diarization, and proposes future improvements
    such as segment analysis, timestamp extraction, and visual element recognition
    to improve multimodal video analysis and search applications. Keywords include
    Gemini, multimodal AI, structured extraction, travel recommendations, YouTube
    video analysis, Pydantic, Python, and video content understanding.
blog/posts/open_source.md:
  hash: b3cb29bb72d1746982e2bb01087f8cdf
  summary: This article explores Instructor's enhanced capabilities for integrating
    with a variety of open source and local large language models (LLMs), including
    OpenAI, Ollama, llama-cpp-python, Groq, Together AI, and Mistral. It highlights
    how Instructor supports structured data extraction and outputs through JSON mode
    and JSON schema, utilizing Pydantic for data validation. Key features include
    model patching, multi-platform compatibility, and simplified API interactions
    for in-process and remote models. The content emphasizes adaptability in AI workflows,
    offering practical code examples for implementing structured outputs with different
    providers, aiming to streamline AI development and improve model control. Core
    keywords include Instructor, structured outputs, LLMs, OpenAI, Pydantic, JSON
    schema, Ollama, llama-cpp-python, Groq, Together AI, Mistral, API integration,
    local models, AI development.
blog/posts/openai-distilation-store.md:
  hash: f192d6f81e391bb953541405d9656871
  summary: OpenAI's API Model Distillation with Instructor enables developers to create
    smaller, efficient, and specialized AI models tailored to specific tasks. By combining
    Instructor's structured output capabilities with API Model Distillation, users
    can produce validated, consistent results while reducing latency and costs. The
    integration supports metadata, proxy kwargs, and seamlessly leverages OpenAI's
    API parameters, enhancing workflow flexibility. This approach improves model efficiency,
    precision, and scalability for AI applications, making it ideal for personalized
    and high-performance implementations. Key words include API Model Distillation,
    Instructor, openAI, structured output, model optimization, AI efficiency, and
    customized AI models.
blog/posts/openai-multimodal.md:
  hash: ce09812cba1cf33f619ecc0cc19a8f18
  summary: OpenAI has introduced audio support in its Chat Completions API with the
    new `gpt-4o-audio-preview` model, enabling developers to process and generate
    both text and audio inputs seamlessly. This feature offers flexible input handling,
    natural, steerable voice capabilities, and integration with tool calling for complex
    workflows. Practical applications include voice-based personal assistants, audio
    content analysis, language learning tools, and accessibility enhancements. Designed
    primarily for asynchronous use, this update expands AI-driven audio and text interactions,
    emphasizing advanced voice processing, customization, and versatile use cases
    in machine learning and AI development.
blog/posts/pairwise-llm-judge.md:
  hash: 306360d9c8a466ffc3083651c8c295df
  summary: The article explores how to create a pairwise LLM judge utilizing the Instructor
    library and Pydantic to evaluate text relevance, demonstrating a practical application
    of structured outputs in language model interactions. It provides a detailed guide
    on setting up the environment, defining a `Judgment` model using Pydantic for
    structured results, and developing a function to assess the relevance between
    a question and a text using OpenAI's GPT-4 model. This tool, beneficial for improving
    search relevance, evaluating question-answering systems, and aiding content recommendation
    algorithms, highlights the potential of combining structured outputs with large
    language models for creating intelligent AI systems. Key concepts include LLM,
    text relevance, AI evaluation, structured outputs, and Pydantic.
blog/posts/parea.md:
  hash: 3384d1bea79b6e46e8b6c9e6681cc1cf
  summary: 'The blog post explores how the Parea platform enhances the OpenAI instructor
    client by improving monitoring, collaboration, testing, and error tracking for
    LLM applications. Core features include automatic grouping of retries into a single
    trace, tracking validation error counts, and providing a UI for labeling JSON
    responses. It demonstrates using Parea with the OpenAI instructor to write emails
    containing links from instructor documentation, emphasizes validation error tracking
    for minimizing costs and latency, and highlights a labeling feature for fine-tuning
    using subject-matter experts. Keywords: Parea, OpenAI, LLM, instructor, validation,
    fine-tuning, error tracking, collaboration.'
blog/posts/pydantic-is-still-all-you-need.md:
  hash: 6e36daa1ff6cdefa634b486b2b61c0b2
  summary: This article emphasizes the importance of using Pydantic for structured
    outputs in language model applications, ensuring data validation, reliability,
    and maintainability. It highlights how Pydantic enables nested objects, custom
    validators, and real-time streaming, enhancing AI-generated content and search
    capabilities. The piece underscores that, despite updates like version 1.0 and
    multi-language support, Pydantic remains a fundamental tool for building robust,
    compatible AI workflows. Key concepts include structured data validation, function
    calling, integrations with various LLM providers, and the future of programming
    with data structures in AI.
blog/posts/rag-and-beyond.md:
  hash: fd37d0f802ff587b57b82fade37522c8
  summary: This article explores enhancing Retrieval Augmented Generation (RAG) by
    integrating query understanding for smarter search solutions. It critiques the
    "Dumb" RAG model that relies heavily on basic embedding search and highlights
    its limitations, such as query-document mismatch and single backend dependency.
    The article introduces advanced RAG models that leverage Pydantic and the instructor
    library to turn user queries into structured, context-aware searches across multiple
    data sources. Key case studies, such as Metaphor Systems and personal assistants,
    illustrate how structured query rewriting and multi-backend dispatch lead to more
    precise and efficient results. Core concepts include RAG, query understanding,
    LLMs, and data modeling with Pydantic.
blog/posts/rag-timelines.md:
  hash: 38763a866b0564e24d4eadb49e515684
  summary: This article explores enhancing retrieval-augmented generation (RAG) systems
    with time filtering using the Python library Instructor and Pydantic models. It
    discusses how to effectively handle time-based constraints in queries, such as
    those asking for information "from the past week." By using Pydantic to model
    time filters and Instructor to integrate large language models (LLMs), developers
    can provide accurate, relevant responses to temporal queries. The article also
    addresses the nuances of handling dates and time zones, emphasizing the importance
    of standardizing and validating these aspects for consistent system performance.
    Key techniques include defining structured output models, prompting LLMs to generate
    query objects, and managing date-related complexities.
blog/posts/semantic-validation-structured-outputs.md:
  hash: c6d1b47e6d426c8bf1a613f36b2eac68
  summary: Semantic validation with LLMs offers a powerful way to ensure structured
    outputs meet complex, subjective, and contextual criteria, surpassing traditional
    rule-based methods. By leveraging LLMs like Instructor's `llm_validator`, developers
    can validate content against nuanced requirements such as tone, style, and adherence
    to community guidelines. This approach is beneficial for applications such as
    content moderation, tone enforcement, and fact-checking, providing a layered validation
    strategy that combines type validation, rule-based checks, and semantic understanding.
    With features like automatic error correction and retries, semantic validation
    enhances output quality and safety in AI-driven applications.
blog/posts/situate-context.md:
  hash: 89cec5544c213f53918318c2b2ba37f9
  summary: 'Learn about implementing Anthropic''s Contextual Retrieval technique to
    enhance Retrieval-Augmented Generation (RAG) systems using async processing for
    performance optimization. The technique addresses context loss when documents
    are chunked, by adding explanatory context before embedding, improving search
    retrieval. The implementation utilizes async processing with Python to process
    document chunks concurrently, achieving significant retrieval failure rate reductions.
    Key features include structured output with Pydantic models, prompt caching, and
    efficient chunking methods. This approach is ideal for optimizing RAG systems
    with improved contextual understanding and retrieval efficiency. Keywords: Contextual
    Retrieval, Async Processing, RAG Systems, Document Chunking, Performance Optimization.'
blog/posts/string-based-init.md:
  hash: f3e9926e6f4ff170c7b494166ca19f9b
  summary: Instructor has introduced a unified provider interface for initializing
    Large Language Model (LLM) providers using a string-based format, simplifying
    the process of switching and experimenting with different models. The `from_provider`
    function supports major providers like OpenAI, Anthropic, Google Gemini, and more,
    offering simplified initialization, consistent syntax, reduced dependency exposure,
    and easy experimentation. It supports both synchronous and asynchronous clients
    and allows mode selection for structured output. The interface respects environment
    variables and provides robust error handling, enhancing usability and flexibility
    for developers. This innovation aims to streamline code portability and reduce
    the learning curve when working with multiple LLM providers.
blog/posts/structured-output-anthropic.md:
  hash: 88a9df95bef4141b8235cdc39fbff07c
  summary: The article discusses how to enhance AI application development using Anthropic's
    Claude with the Instructor library, focusing on structured outputs and prompt
    caching. It explains how to integrate Anthropic's models with Pydantic for creating
    structured data, streamlining the extraction of specific information from AI-generated
    responses. Additionally, the guide highlights the benefits of prompt caching for
    improving response times and reducing costs in applications using large context
    windows. Key features include seamless Anthropics and Instructor integration,
    efficient AI development, and cost-effective strategies for large language models.
blog/posts/tidy-data-from-messy-tables.md:
  hash: bb66ca67fa1b7f8e98d10be0f9aff2e1
  summary: "This article discusses how to convert messy, unstructured tables into\
    \ tidy data using the instructor tool with structured outputs, simplifying data\
    \ cleaning and analysis. It highlights common issues with messy exports\u2014\
    such as merged cells, implicit relationships, and mixed data types\u2014and demonstrates\
    \ how defining custom types and leveraging AI-powered extraction can automatically\
    \ produce clean pandas DataFrames. The approach enables efficient processing of\
    \ multiple tables from images, facilitating seamless integration with data analysis\
    \ and visualization workflows. Key concepts include data tidying, structured outputs,\
    \ pandas, AI-driven data extraction, and productivity in data analysis pipelines."
blog/posts/timestamp.md:
  hash: 1c148db378a535746af59ac0dd3c1cfb
  summary: This article discusses solving timestamp format inconsistencies in video
    content parsing using Pydantic for data validation and a custom parser. It addresses
    the challenge of varying timestamp formats like "HH:MM:SS" and "MM:SS," which
    can cause errors in language model outputs, especially in video processing and
    NLP tasks. The solution involves defining expected formats and using a custom
    validator to normalize timestamps to a consistent "HH:MM:SS" structure, which
    reduces ambiguity and parsing errors. This method offers a robust framework for
    handling this common issue, outperforming alternative approaches like constrained
    sampling and simple JSON schema validation. The post includes test cases to demonstrate
    the solution's effectiveness. Key terms include timestamp, Pydantic, data validation,
    video processing, and NLP.
blog/posts/using_json.md:
  hash: c38638ce4dbfc143d9de932bda098e96
  summary: Instructor is a Python library that simplifies extracting well-structured
    JSON data from Large Language Models (LLMs) like GPT-3.5, GPT-4, and open-source
    models using Pydantic models. It offers seamless integration with the OpenAI SDK,
    enabling developers to map LLM outputs to validated, type-enforced JSON structures
    with minimal syntax learning. Instructor emphasizes ease of use, validation, and
    serialization, making it ideal for working with complex JSON data in LLM applications.
    Key features include support for multiple programming languages, validation, retries,
    streaming responses, and compatibility with various LLM platforms, making it a
    powerful tool for developers seeking reliable JSON output extraction from LLMs.
blog/posts/validation-part1.md:
  hash: 73f4d38758e70dd910be77fe3289a874
  summary: This article explores the evolution of data validation from static, rule-based
    methods to dynamic, machine learning-driven approaches using Python, Pydantic,
    and Instructor. It highlights how LLM-powered validators enable adaptive content
    moderation, chain of thought validation, and source citation verification, enhancing
    AI reliability. Key concepts include probabilistic validation, custom field validators,
    model validators, validation context, and retry mechanisms. The content emphasizes
    leveraging LLMs for self-correction, improving validation accuracy, and ensuring
    responsible AI outputs, making it a valuable resource for developers interested
    in AI validation, Python, Pydantic, and machine learning integration.
blog/posts/version-1.md:
  hash: a3436323e8334df26966f3b6ecf07788
  summary: The announcement introduces Instructor 1.0.0, a simplified API for interfacing
    with OpenAI that enhances usability by providing improved typing support, data
    validation, and streamlined integration while maintaining compatibility with existing
    standards. Key features include the introduction of `instructor.from_openai` for
    client creation, consistent handling of default arguments, and support for type
    inference with methods like `create_with_completion`, `create_partial`, and `create_iterable`.
    With robust validation and error handling, the tool is designed to support multiple
    languages, maintaining ease of use across platforms. Popular amongst developers,
    Instructor boasts over 4000 GitHub stars and 120k monthly downloads. Key keywords
    include API Development, OpenAI, Data Validation, Python, and LLM Techniques.
blog/posts/why-care-about-mcps.md:
  hash: 12f0fc031ffca52b4b3526c950d51777
  summary: "The article provides a detailed overview of the Model Context Protocol\
    \ (MCP), a standardized protocol developed by Anthropic to facilitate the interaction\
    \ between AI models and external systems. It highlights the importance of MCP\
    \ in solving integration challenges by transforming the complex M\xD7N problem\
    \ into a simplified M+N problem, allowing seamless integration of AI applications\
    \ with various tools. The article compares MCP with OpenAPI, underscoring MCP's\
    \ role in enabling AI models to autonomously discover and utilize tools with semantic\
    \ understanding, as opposed to OpenAPI's focus on human developers. Additionally,\
    \ it outlines growing adoption, development tips, and the practical applications\
    \ of MCP with platforms like Claude Desktop, Cursor, and OpenAI's Agent SDK. Keywords\
    \ include Model Context Protocol, MCP, AI integration, OpenAI, Anthropic, OpenAPI,\
    \ and AI standardization."
blog/posts/writer-support.md:
  hash: 90cad38cf2523db99ce9dd0f6d00fcb3
  summary: The article announces the integration of Writer's enterprise-grade LLMs,
    including the Palmyra X 004 model, with the Instructor platform to enable structured
    outputs and enterprise AI workflows. It explains how to set up the integration,
    generate structured data extraction, and stream responses for improved responsiveness.
    Key features include automatic request retries, support for async processing,
    and usage examples for data extraction, classification, and validation. Keywords
    include Writer, Instructor, enterprise AI, structured outputs, Palmyra X 004,
    API integration, streaming, retries, and AI workflows.
blog/posts/youtube-flashcards.md:
  hash: 21a725c2c361979009b2446c134be1f0
  summary: 'This guide demonstrates how to leverage Language Model Meta (LLM) tools
    like Instructor and Burr to generate interactive flashcards from YouTube transcripts,
    enhancing learning across various subjects. It explains the process of defining
    structured response models with Instructor, retrieving transcripts using youtube-transcript-api,
    and creating question-answer pairs with GPT-4.ombined with Burr''s modular application
    framework, users can build robust, observable, and debug-friendly flashcard apps.
    Key features include defining actions and transitions for workflow management,
    integrating telemetry with OpenTelemetry, and deploying web or notebook-based
    interfaces. This approach enables reliable AI-driven content creation, complex
    agent building, and observability for educational tools and knowledge extraction
    applications. Keywords: LLM, Instructor, Burr, flashcards, YouTube transcripts,
    question-answer generation, OpenAI, automation, observability, AI education.'
blog/posts/youtube-transcripts.md:
  hash: f6904e13b76dc8a15942b76c76104f90
  summary: This article outlines how to extract and summarize YouTube video transcripts
    into structured chapters using Python, Pydantic, and OpenAI's GPT models. It demonstrates
    how to fetch transcripts with the `youtube_transcript_api`, define Pydantic models
    for chapters and other content types, and generate detailed chapter summaries
    with AI. The tutorial focuses on analyzing video content, creating adaptable data
    models for study notes, content summaries, and quizzes, enhancing content organization
    and application development for video summarization, data processing, and AI-powered
    content analysis. Key keywords include YouTube transcripts, Python, Pydantic,
    GPT, data processing, video summarization, and AI applications.
cli/batch.md:
  hash: a17e7a3a3fd64e41ebccd5540036a157
  summary: The guide provides an overview of managing batch jobs using the OpenAI
    Command Line Interface (CLI), enabling efficient operation on both OpenAI and
    Anthropic platforms. It covers key functionalities like creating, listing, and
    canceling batch jobs, as well as downloading job results. Users can switch between
    OpenAI and Anthropic using the `--use-anthropic` flag. The tutorial includes processes
    such as creating batch jobs from JSONL files and provides detailed command examples,
    helping users leverage robust batch processing capabilities for tasks like email
    classification. Key terms include OpenAI CLI, batch job management, Anthropic,
    and command line interface.
cli/finetune.md:
  hash: 7deb0a4241365be22ce804c7c21908bf
  summary: The article "Managing Fine-Tuning Jobs with the Instructor CLI" provides
    a comprehensive guide on how to use the Instructor CLI to manage fine-tuning jobs
    with OpenAI. It covers essential commands and options for creating, viewing, and
    managing these jobs, such as creating a fine-tuning job from a file using `create-from-file`,
    or from an existing ID with `create-from-id`. Key functionalities include monitoring
    job statuses, uploading necessary files, and adjusting fine-tuning parameters
    like model choice, number of epochs, and learning rate. Additionally, the article
    encourages contributions to the CLI's development, highlighting areas open for
    enhancement and collaboration. Relevant keywords include fine-tuning jobs, Instructor
    CLI, OpenAI, model training, and command line interface.
cli/index.md:
  hash: 8331441083b208ef53688aa8ca292269
  summary: 'The Instructor CLI Tools offer a suite of command-line utilities designed
    to enhance workflows when using OpenAI''s API by monitoring usage, fine-tuning
    models, and accessing documentation. Key features include commands for tracking
    API usage and costs, creating and managing fine-tuned models, and quick access
    to documentation directly from the terminal. Users can install the tools via `pip
    install instructor` and must set the OpenAI API key as an environment variable.
    Additional resources and support are available through GitHub and the community
    Discord. Keywords: Instructor CLI Tools, command-line utilities, OpenAI API, usage
    monitoring, model fine-tuning, documentation access.'
cli/usage.md:
  hash: 95aa3f140fe59a144287c98679c27c15
  summary: 'The OpenAI API Usage CLI Guide provides detailed instructions on monitoring
    OpenAI API usage using a command-line interface tool. This tool allows users to
    track API usage by model, date, and cost, offering commands like `list` to display
    usage data over the past few days. Key features include listing usage for a specified
    number of days and checking today''s usage. The guide also invites users to contribute
    to the development of this utility via GitHub. Keywords: OpenAI API, CLI tool,
    API usage monitoring, command-line interface, OpenAI models, usage tracking, GitHub
    contribution.'
concepts/alias.md:
  hash: 8c7fc8fbbe513d178333a7986a8227bb
  summary: This overview highlights the use of aliases in Pydantic for improved data
    validation and model serialization. It explains how aliases enable mapping between
    external data field names and internal model attributes, facilitating seamless
    data parsing. The page emphasizes exploring Pydantic's latest features and documentation
    related to aliases, essential for efficient data handling and validation in Python
    applications. Key concepts include alias definition, usage, and best practices
    for leveraging aliases to enhance data model flexibility.
concepts/caching.md:
  hash: ac0e8043ff4b03799692dbd4910d2e64
  summary: This guide explores various Python caching techniques including in-memory,
    disk-based, and Redis caching to optimize application performance. It covers the
    use of `functools.cache` for simple in-memory caching, ideal for small to medium
    applications with immutable arguments. Additionally, it demonstrates persistent
    caching with `diskcache` and distributed caching with Redis, both utilizing a
    shared `instructor_cache` decorator that serializes Pydantic models for efficient
    data storage. Key concepts include cache invalidation considerations, cache key
    generation, and serialization techniques, making these methods suitable for reducing
    computation time, handling large datasets, and supporting scalable, distributed
    systems. Core keywords include Python caching, in-memory cache, diskcache, Redis,
    Pydantic, cache decorators, performance optimization, and persistent storage.
concepts/dictionary_operations.md:
  hash: 0b5816386b8ed615bb4a8c30a0afccbc
  summary: "This document outlines performance optimizations for dictionary operations\
    \ in Instructor, focusing on enhancing message extraction, retry functions, message\
    \ handler selection, and system message handling. Key improvements include replacing\
    \ nested `get()` calls with direct key lookups, pre-extracting variables, using\
    \ conditional logic instead of large mappings, and caching type checks for efficiency.\
    \ Benchmark results demonstrate significant speed improvements\u2014up to 62%\u2014\
    without altering functionality. These optimizations aim to boost Instructor's\
    \ high-throughput performance while maintaining API consistency, making it ideal\
    \ for efficient LLM message passing, configuration management, and scalable AI\
    \ applications."
concepts/distillation.md:
  hash: 88f400b35fb27b4235f08e4c61053267
  summary: 'The article introduces Instructor''s `Instructions` library for seamless
    fine-tuning of Python functions with language models like GPT-3.5-turbo. It explains
    how to automate dataset creation for model training by annotating functions that
    return Pydantic objects, simplifying the fine-tuning process, and logging outputs
    for efficient data management. The approach enables distilling function behavior
    into model weights, facilitating backward compatibility and model-switching via
    the `dispatch` mode. Key features include streamlined data preparation, automatic
    dataset generation, and easy integration for function-level fine-tuning, making
    Instructor a powerful tool for optimizing language models in Python applications.
    Keywords: Instructor, Instructions, fine-tuning, Python functions, language models,
    GPT-3.5, distillation, Pydantic, model training, dataset automation, function
    calling, backward compatibility.'
concepts/enums.md:
  hash: 727e8787171ecd5104e0689e1d83184c
  summary: The article discusses using Enums and Literals in Pydantic for effective
    role management, highlighting their role in preventing data misalignment by standardizing
    user roles. Key topics include the implementation of Enums with a fallback "Other"
    option to handle uncertainties, and an alternative approach using Literals for
    role definitions. Core ideas emphasize the importance of standardization and flexibility
    in model design, specifically for roles like "PRINCIPAL", "TEACHER", "STUDENT",
    and "OTHER". Keywords include Enums, Literals, Pydantic, role management, data
    standardization, and fallback options.
concepts/error_handling.md:
  hash: 5007d7c8abe6942912b823c5e9d22130
  summary: This guide on Error Handling in Instructor provides a comprehensive overview
    of managing exceptions and errors when using Instructor for structured outputs.
    It details the exception hierarchy, including `InstructorError` and specific exceptions
    like `IncompleteOutputException`, `InstructorRetryException`, `ValidationError`,
    `ProviderError`, `ConfigurationError`, `ModeError`, and `ClientError`. The content
    offers best practices for catching specific exceptions, handling provider and
    configuration errors, logging, graceful degradation, and integrating hooks for
    error monitoring. Key concepts include exception hierarchy, error handling strategies,
    provider setup issues, validation failures, mode errors, and retry logic, ensuring
    robust and resilient use of Instructor for AI model integrations. Keywords include
    Instructor error handling, exceptions, validation, retries, provider errors, configuration
    issues, hooks, and debugging.
concepts/fastapi.md:
  hash: 4a9d66d0b46d7f503078520ae02f08fa
  summary: 'This guide explores how to integrate Pydantic models with FastAPI for
    efficient API development. FastAPI is a high-performance Python web framework
    known for its seamless Pydantic integration, automatic OpenAPI documentation,
    and JSON Schema validation. The article provides code examples demonstrating how
    to start a FastAPI app with POST requests, handle data with Pydantic models, and
    implement streaming responses using FastAPI and large language models (LLMs).
    Key features include automatic interactive API documentation accessible via a
    `/docs` page, making API testing straightforward. SEO Keywords: FastAPI, Pydantic
    models, API development, Python, OpenAPI, JSON Schema, streaming responses, AsyncIO.'
concepts/fields.md:
  hash: f3d413417c1bc9ebdd05f917226cf988
  summary: 'This guide details how to customize Pydantic models using the `Field`
    function to add metadata, set default values with `default` and `default_factory`,
    and utilize `Annotated` for enhanced type annotations. It explains controlling
    JSON schema generation through parameters like `title`, `description`, `examples`,
    `json_schema_extra`, and techniques to exclude fields with `exclude` or skip them
    in schemas using `SkipJsonSchema`. Key features include schema customization for
    prompt engineering, omitting irrelevant fields, and managing schema metadata for
    effective API validation and language model integration. Keywords: Pydantic, Field
    metadata, JSON schema customization, default values, exclude fields, schema annotations,
    prompt engineering.'
concepts/hooks.md:
  hash: 5c8033a493a0df9463f2d5859e78e8eb
  summary: 'The article delves into the use of hooks within the Instructor library
    for enhancing functionality such as logging, error handling, and custom behaviors
    during API interactions. With the Hooks system managed by the `Hooks` class, users
    can register and utilize predefined events like `completion:kwargs`, `completion:response`,
    and error-handling hooks such as `completion:error` and `parse:error`. It details
    how to implement, manage, and clear hooks, supports type safety with Python''s
    Protocol, and also suggests using hooks in testing scenarios. The content highlights
    the integration with OpenAI and emphasizes customizable event handling, offering
    users flexibility and better error monitoring in their applications. Keywords
    include: Instructor library, hooks, event handling, logging, error handling, API
    interactions, OpenAI.'
concepts/index.md:
  hash: 919fa2e62c009c62d3c668ca1d119976
  summary: The Instructor library is a comprehensive toolkit for building structured,
    validated, and efficient interactions with large language models (LLMs). It leverages
    Pydantic models for defining data schemas, enhances LLM clients through patching,
    and offers features for validation, retrying, and streaming responses. Core concepts
    include model definition, prompting, multimodal support, data handling, error
    management, and performance optimization through caching and parallel processing.
    Instructor facilitates seamless integration with frameworks like FastAPI and supports
    dynamic templating and distillation for production environments. Key keywords
    include LLM, Pydantic, prompting, validation, streaming, caching, fastapi, error
    handling, and structured outputs.
concepts/iterable.md:
  hash: c2dfb4f339a4f8ae27989d2da894d637
  summary: The article provides a tutorial on extracting structured data using Python,
    focusing on the use of Iterable and streaming capabilities with Pydantic and OpenAI.
    It highlights the advantages of using the `create_iterable` method for processing
    structured objects from a single LLM call, especially in entity extraction and
    multi-task outputs. The tutorial includes examples of synchronous and asynchronous
    usage, demonstrating how to extract data like user details or weather information.
    The `create_iterable` method is recommended for its simplicity and reduced error
    potential compared to manually specifying `Iterable[...]` with `stream=True`.
    Keywords include structured data extraction, Python, Iterable, streaming, Pydantic,
    OpenAI, `create_iterable`, synchronous, and asynchronous usage.
concepts/lists.md:
  hash: 87115c5871b7f897999d87d86cd68cbd
  summary: This article explores advanced techniques for structured data extraction
    in Python using iterable and streaming capabilities with Pydantic and OpenAI.
    It demonstrates how to define schemas and utilize `Iterable[T]` for multi-task
    extraction, enabling dynamic class creation, prompt generation, and efficient
    token streaming. The guide also covers synchronous and asynchronous streaming
    methods, showcasing examples with GPT-3.5 and GPT-4 models. Key concepts include
    data serialization, real-time token processing, and leveraging instructor's API
    for scalable, schema-based entity extraction in Python, making it ideal for developers
    working on AI-driven data parsing and automation.
concepts/logging.md:
  hash: c320e557ce8c76ec024a7048d8c00b58
  summary: This guide demonstrates how to enable detailed debugging of OpenAI API
    requests and responses in Python using the DEBUG logging level. It shows how to
    set up logging to capture comprehensive request and response data, facilitating
    efficient troubleshooting and understanding of interactions with OpenAI's models.
    The example includes logging configuration, invoking chat completions with custom
    response models, and example debug output illustrating how to monitor request
    parameters, internal processing steps, and API response details. Key topics include
    OpenAI API debugging, Python logging, request/response monitoring, and improving
    development workflows for AI integrations.
concepts/maybe.md:
  hash: 4e245b781d8f282eb06813ed10498526
  summary: The article explores the implementation of the Maybe pattern for error
    handling in functional programming using Python's Pydantic library. It focuses
    on how the Maybe pattern can encapsulate results and potential errors without
    resorting to exceptions or returning `None`, enhancing robust error handling.
    The pattern is implemented in a Pydantic `MaybeUser` class, which includes fields
    for the result, error status, and error message. This approach is particularly
    useful for language model (LLM) calls, reducing hallucinations. A practical example
    is provided, demonstrating how the pattern is used to extract user details from
    text inputs. Key topics include functional programming, error handling, Pydantic,
    Maybe pattern, and structural pattern matching.
concepts/models.md:
  hash: 14c6638223e145cb56f78b01ad3c745f
  summary: This article explains how to use Pydantic for defining dynamic and static
    response models for Large Language Models (LLMs), including creating schemas with
    `BaseModel`, optional values, and runtime model generation with `create_model`.
    It highlights how to use prompt annotations and docstrings for prompt generation,
    validate API responses, and add custom behaviors or methods to models. Key concepts
    include dynamic model creation based on database or configuration data, omitting
    fields from prompts, and integrating custom logic for tailored LLM responses,
    making Pydantic a flexible tool for managing LLM output schemas and response validation.
concepts/multimodal.md:
  hash: 6b81751a99a294b562c47fcef3e3f496
  summary: 'The article discusses Instructor''s seamless multimodal interface for
    handling images, PDFs, and audio files across various AI models like OpenAI, Anthropic,
    Google GenAI, and Mistral. Key features include creating media instances from
    URLs, file paths, and base64 strings, alongside automatic provider-specific formatting,
    ensuring clean, adaptable code. The Image, Audio, and PDF classes simplify interaction
    by abstracting differences among AI providers, while additional features like
    Anthropic prompt caching and Google GenAI file support enhance functionality.
    This comprehensive approach streamlines application development, emphasizing consistency,
    efficiency, and adaptability across AI technologies. Key terms: multimodal interface,
    AI models, image analysis, PDF parsing, audio processing, Anthropic caching, Google
    GenAI, Instructor API.'
concepts/parallel.md:
  hash: ef1722f94742cadf3b5dbfa93d7c62f1
  summary: OpenAI's experimental Parallel Function Calling enables developers to call
    multiple functions simultaneously within a single request, significantly reducing
    application latency. Supported currently by Google and OpenAI, this feature allows
    for efficient execution of tools such as weather data retrieval and web searches
    without needing complex parent schemas. Using specific modes like `PARALLEL_TOOLS`
    for OpenAI and `VERTEXAI_PARALLEL_TOOLS` for Vertex AI, developers can specify
    response models as iterables of multiple object types (e.g., Weather, GoogleSearch).
    Key concepts include reduced latency, parallel tool execution, and dynamic response
    handling with Pydantic models, making it an important optimization for AI-powered
    applications.
concepts/partial.md:
  hash: d8cf2df0b922d2a39bf024aeabca278e
  summary: This article explains how to use instructor and OpenAI for streaming partial
    responses in Python, enabling incremental model outputs suitable for real-time
    applications like UI rendering. It covers field-level streaming with `create_partial`,
    handling incomplete data with `PartialLiteralMixin`, and managing response models
    as generators that yield progressive updates. The guide highlights limitations
    such as unsupported validators during streaming and provides practical examples,
    including extracting conference information with asynchronous streaming support.
    Key concepts include field-level partial responses, model streaming, generator-based
    incremental updates, and integration with OpenAI's APIs for real-time data processing.
concepts/patching.md:
  hash: 73bf8b99f5d3d3eb6601921d99f93932
  summary: The document discusses how the Instructor tool enhances Large Language
    Model (LLM) client libraries by patching them to support structured outputs. Core
    features include adding parameters like `response_model`, `max_retries`, and `validation_context`
    to methods in the client, enabling structured responses. It outlines different
    patching modes such as TOOL, GEMINI, and JSON for various LLM providers like OpenAI
    and Gemini, helping ensure compatibility and improved data handling. Patching
    is aimed at facilitating stable tool calling, managing validations, and providing
    JSON outputs. Keywords include structured output, LLM client libraries, Instructor
    tool, OpenAI, Gemini, patching, and tool calling.
concepts/philosophy.md:
  hash: 34c62573bd722ec5dd7c1c2f3c0ae9b5
  summary: Instructor is a streamlined AI framework designed to simplify the integration
    of language models without introducing unnecessary complexities. Emphasizing the
    philosophy that "simple beats complex," it leverages familiar tools such as Pydantic,
    ensuring developers can use what they already know without new abstractions or
    extensive manuals. Instructor offers flexibility by allowing easy reversion to
    raw API usage, enhancing debuggability and composability with minimal lock-in.
    By focusing on simple, function-driven design over configuration-heavy approaches,
    Instructor enables seamless interaction with models like OpenAI's GPT-4, making
    structured LLM outputs as straightforward as defining a Pydantic model. This approach
    ensures a tiny API surface, zero vendor lock-in, and a Pythonic feel. Key features
    include easy data extraction, integration with LLMs, and maintaining control over
    your code and its logic.
concepts/prompt_caching.md:
  hash: 580600c0f70f02c1892b24456a32cdcc
  summary: Prompt caching is an optimization feature in OpenAI and Anthropic APIs
    that enhances performance and reduces costs by caching shared prompt segments.
    In OpenAI, prompt caching works automatically for models like gpt-4o and gpt-4o-mini
    with prefix matching, requiring no code changes. Anthropic's prompt caching, now
    generally available, necessitates explicit use of the `cache_control` parameter
    and is especially beneficial for large prompts exceeding token minimums (2048
    tokens for Claude Haiku, 1024 for Claude Sonnet). This feature significantly lowers
    response times and costs by enabling cache reuse during multiple API calls, making
    it essential for efficient, large-scale language model applications. Key keywords
    include prompt caching, API optimization, OpenAI, Anthropic, cost reduction, response
    time, model models, cache management, and large prompt handling.
concepts/prompting.md:
  hash: e27dde9b271c8c6944f53125f39a0042
  summary: The article provides a comprehensive guide on effective prompt engineering
    using Pydantic and Instructor, focusing on enhancing modularity, flexibility,
    and data integrity in Python models. Key strategies include designing self-descriptive
    and reusable components, employing enums and literals for standardization, and
    handling errors with the Maybe pattern. The guide also recommends using optional
    attributes, reiterating long instructions, managing list lengths, and defining
    entity relationships to improve data quality. By incorporating these practices,
    developers can ensure better structure, clarity, and maintainability in their
    applications.
concepts/raw_response.md:
  hash: 44557d68c40cf4d99ef68b41047544ef
  summary: This guide provides a tutorial on creating custom models using OpenAI's
    API with Python. It specifically demonstrates how to use the `instructor` library
    to extract user data efficiently by integrating OpenAI's GPT model, such as "gpt-3.5-turbo,"
    with Pydantic for response validation. The example illustrates extracting user
    attributes like name and age from a text input using the `UserExtract` model.
    Additionally, the tutorial explains accessing raw responses from Anthropic models
    for debugging purposes. Key concepts include OpenAI completions, data extraction,
    custom client, and Pydantic models.
concepts/reask_validation.md:
  hash: eda13e17af5b47f10ddff3a58680307f
  summary: This article explores enhancing AI validation processes using Pydantic's
    flexible validation framework for both code-based and LLM-based outputs. Key techniques
    include defining custom validators, leveraging reasking with retry mechanisms,
    and advanced validation methods like model-level validation and context-aware
    checks. It emphasizes improving AI output accuracy, handling validation errors
    effectively, and optimizing token usage by disabling URL links in error messages.
    Core keywords include Pydantic, AI validation, LLM validation, reasking, validation
    errors, JSON decoding, token optimization, and autonomous system improvement.
concepts/retrying.md:
  hash: c1d7d6678c4c53737e6238bf528bbc1e
  summary: The article provides a comprehensive guide on implementing effective retry
    logic in Python using Pydantic and Tenacity, aimed at enhancing robust application
    behavior. It illustrates the ease of defining validators with Pydantic and demonstrates
    simple and advanced retry strategies with examples, such as using `max_retries`,
    handling exceptions, and configuring retry logic with Tenacity. Key features include
    global timeout support and custom retry callbacks for logging and debugging. The
    guide also covers asynchronous retries using `AsyncRetrying` and diverse retry
    capabilities offered by Tenacity, making it an essential resource for developers
    needing reliable error handling and validation in Python applications. Key terms
    include Pydantic, Tenacity, retry logic, validators, and asynchronous retries.
concepts/semantic_validation.md:
  hash: 742c692748d307dddfc65be313d58d8a
  summary: This guide explores semantic validation using large language models (LLMs)
    in Instructor, enabling advanced content validation beyond traditional rule-based
    methods. It highlights how LLMs can assess complex, subjective, and contextual
    criteria such as tone, style, content relevance, fact-checking, and policy enforcement.
    The approach leverages the `llm_validator` function for natural language-based
    validation, supporting multi-field, content moderation, and dynamic validation
    patterns. Key concepts include leveraging LLMs for nuanced validation, balancing
    cost and performance, and best practices for implementing effective semantic validation
    systems. Keywords include natural language validation, LLM-powered validation,
    Instructor platform, content moderation, policy enforcement, fact-checking, multi-field
    validation, and dynamic criteria.
concepts/templating.md:
  hash: 8b3f459aae3b028d9cdfc85a670095de
  summary: This guide explores effective prompt templating using Jinja and Pydantic
    to create dynamic, secure, and maintainable prompts for AI models. It highlights
    how to pass context variables for prompt rendering and validation, implement complex
    logic with Jinja syntax, and integrate Pydantic validators for context-aware validation,
    including handling sensitive data with SecretStr. Emphasis is placed on security
    through sandboxed Jinja environments and best practices for managing sensitive
    information, enabling flexible, secure, and scalable prompt engineering for AI
    applications. Key keywords include prompt templating, Jinja, Pydantic, context
    variables, validation, security, secrets, and dynamic prompts.
concepts/typeadapter.md:
  hash: 40fefdf3e9f6d305e1c2280d9fc8b944
  summary: This page provides an overview of Pydantic's Type Adapter concepts, detailing
    ongoing updates and developments. It highlights the core ideas of adapting and
    customizing data validation and serialization using Pydantic's type system. The
    page serves as a work in progress, directing users to the official Pydantic documentation
    for latest information on Type Adapters, a key feature for flexible data modeling
    and type management. Key keywords include Pydantic, Type Adapter, data validation,
    type customization, and Python data modeling.
concepts/typeddicts.md:
  hash: 81e543be61c6eae101e7f1fc5bd324ec
  summary: The document provides a tutorial on using TypedDicts in Python when working
    with the OpenAI API for structured data responses. It explains how to define a
    TypedDict class to specify structured data types, such as strings and integers,
    and demonstrates its integration with the OpenAI API through the `instructor`
    library. The example provided showcases the creation of a structured response
    model, using a `User` TypedDict to parse a response from the GPT-3.5-turbo model,
    highlighting ease of use and strong typing for better handling API responses.
    Key concepts include Python TypedDicts, OpenAI API integration, structured data
    handling, and typed responses.
concepts/types.md:
  hash: 4399736e0701f581b37e9ba09635169b
  summary: The article "Working with Types in Instructor" explores how to effectively
    utilize various data types in the Instructor platform, enhancing structured outputs
    from basic primitives to complex structures. Key elements include the use of simple
    types such as `str`, `int`, `float`, and `bool`, as well as complex types like
    `List`, `Dict`, `Union`, `Literal`, and `Enum`. It covers how to employ `pydantic.BaseModel`
    for structuring data and emphasizes the use of `typing.Annotated` for adding context
    and descriptions. The article also delves into advanced examples, such as converting
    markdown data to a pandas DataFrame and using lists of unions for diverse response
    types. These concepts are illustrated with practical code snippets, highlighting
    the versatility and capabilities of the Instructor framework in managing various
    data types for better API response modeling. Keywords include Instructor, data
    types, Pydantic, Python, structured outputs, and API response modeling.
concepts/union.md:
  hash: d19fc6ce0a547f93d856b9a2a64f2f16
  summary: 'This page explains how to implement Union types in Pydantic models to
    manage multiple action types in Python applications. It highlights best practices
    for using Union types to enable flexible data validation and modeling, allowing
    models to accept different data structures. The content emphasizes handling diverse
    input scenarios effectively with Pydantic''s Union feature, providing valuable
    guidance for developers working with complex data validation and type hinting.
    Key keywords include Union types, Pydantic models, data validation, Python, type
    hints, and flexible data handling. Note: the original page has been consolidated
    into a comprehensive Union Types guide for more detailed information.'
concepts/unions.md:
  hash: eaaf35658f139d7cce326903aad2e9c2
  summary: This guide explores the use of Union types in Instructor to handle multiple
    response formats from language models, emphasizing core concepts like basic, discriminated,
    and nested unions, as well as optional fields. It covers best practices for type
    hints, validation, and documentation, along with practical patterns such as multiple
    response types and dynamic action selection. The content highlights integrating
    Union types with Instructor for validation, streaming, error handling, and type
    checking, providing key examples and workflows for building flexible, robust LLM-based
    applications. Key words include Union types, Instructor, Pydantic, response models,
    discriminated unions, validation, streaming, error handling, dynamic actions,
    AI models, OpenAI, and type safety.
concepts/usage.md:
  hash: 80711f0189c13e1c0625c56bf2b16f58
  summary: 'This guide explains how to handle non-streaming requests in OpenAI using
    Python, with a focus on tracking token usage and managing exceptions. It demonstrates
    accessing raw response data to monitor token consumption, including detailed usage
    metrics like prompt and completion tokens. The content also covers handling the
    IncompleteOutputException, which occurs when the context length is exceeded, by
    catching the exception and adjusting the prompt accordingly. Key concepts include
    OpenAI API, usage tracking, token management, error handling, and Python implementation.
    Keywords: OpenAI, non-streaming requests, token usage, completion metrics, IncompleteOutputException,
    Python, API management.'
concepts/validation.md:
  hash: 6b6c0a71930fe29b7fbb38cf315630f8
  summary: This comprehensive guide on validation in Instructor explains how to ensure
    LLM outputs conform to predefined schemas using Pydantic. It covers core validation
    techniques such as type checking, data coercion, custom validators, and field
    constraints, along with advanced semantic validation leveraging LLMs for content
    moderation, fact-checking, and complex rule-based validation. The guide highlights
    best practices, error handling, nested validation, optional fields, and complex
    validation patterns, providing practical code examples and strategies for maintaining
    data consistency, type safety, and business logic enforcement in LLM-driven applications.
    Keywords include Instructor validation, Pydantic, semantic validation, data validation,
    LLM validation, error handling, custom validators, content moderation, fact-checking,
    content policy, and validation best practices.
contributing.md:
  hash: 7e259c9181ec17b13df76c6f492cede8
  summary: Contributing to Instructor involves testing and evaluating AI models with
    evals, reporting issues, submitting pull requests, and enhancing documentation.
    The guide details setup with tools like UV and Poetry, adding support for new
    LLM providers, and maintaining code quality through style guidelines and conventional
    commits. Developers can participate by creating evals, troubleshooting bugs, and
    improving the open-source instructor library via GitHub, with resources for documentation,
    code standards, and community collaboration. Keywords include open-source, AI,
    evals, LLM, GitHub contributions, Python, documentation, code style, and development
    workflow.
examples/action_items.md:
  hash: 330c78a61f002ff6c56b77dda4ac62bf
  summary: This article explains how to automate the extraction of action items from
    meeting transcripts using OpenAI's API and Pydantic. It details modeling action
    items as Ticket objects with subtasks, priorities, assignees, and dependencies,
    enabling efficient project management. The guide includes code examples for generating
    actionable tasks from transcripts, visualizing data with Graphviz, and emphasizes
    the importance of automating task identification to improve productivity and prevent
    overlooked responsibilities in meetings. Key keywords include action item extraction,
    meeting transcripts, OpenAI API, Pydantic, project management automation, task
    dependency, and GPT-4.
examples/audio_extraction.md:
  hash: a797125b7d4354a5f8c32d687567228b
  summary: 'This guide demonstrates how to use OpenAI''s GPT-4 audio models with Instructor
    to extract structured information from audio files, such as transcribing voice
    data into predefined data models. It includes steps for encoding audio, configuring
    the model, and parsing the output into Pydantic structures like Person with name
    and age. Key features include processing WAV files, leveraging multimodal capabilities,
    and applications like voice data extraction, automated form filling, and transcription.
    Essential keywords: audio information extraction, OpenAI GPT-4, instructor library,
    structured data, Pydantic, multimodal AI, voice processing, audio transcription.'
examples/batch_classification_langsmith.md:
  hash: 996b30c651684530af4333e94df8f6a7
  summary: This article explains how to enhance the OpenAI client with LangSmith and
    Instructor for improved observability, monitoring, and functionality in LLM applications.
    It demonstrates integrating LangSmith's SDK with OpenAI's chat completion API,
    using features like client wrapping and rate limiting. The guide also showcases
    applying Instructor to patch the client in TOOL mode, enabling additional capabilities.
    Key topics include LangSmith, OpenAI client integration, Instructor, rate limiting,
    question classification, and application monitoring, making it ideal for developers
    seeking scalable, observable AI solutions.
examples/batch_job_oai.md:
  hash: d13fc5a068b73df1e50ff653f20588b5
  summary: This guide explains how to efficiently generate large-scale synthetic question-answer
    pairs using OpenAI's Batch API with Instructor. It covers creating JSONL files
    from datasets like ms-marco, leveraging batch jobs for cost-effective and high-rate
    data generation, and managing batch workflows through CLI commands. Key features
    include using Pydantic models for response parsing, handling batch job creation,
    monitoring progress, and downloading results. Important keywords include synthetic
    data generation, OpenAI Batch API, Instructor, large-scale datasets, ms-marco,
    question-answer pairs, cost-effective AI workflows, and data parsing.
examples/building_knowledge_graphs.md:
  hash: 4055c02b7485da53099015c6d456b1fc
  summary: This tutorial offers a comprehensive guide to building knowledge graphs
    from textual data using OpenAI's API and Pydantic. It demonstrates how to extract
    structured information from unstructured text, such as identifying entities and
    relationships, and representing them as nodes and edges in a graph. The example
    includes Python code for defining graph models with Pydantic, integrating OpenAI's
    API for text processing, and generating visualizable knowledge graphs. Key concepts
    include automated knowledge graph construction, natural language processing, entity
    and relationship extraction, and Python implementation, making it an essential
    resource for data scientists and developers interested in semantic data modeling
    and knowledge graph automation.
examples/bulk_classification.md:
  hash: 21849e9a44f226f43e8b94a17846fa12
  summary: 'This tutorial provides a comprehensive guide on implementing user-provided
    tag classification using FastAPI, Pydantic models, and the OpenAI API with async
    functions for parallel processing. It emphasizes defining flexible tag schemas
    with identifiers, instructions, and optional confidence scores, as well as validating
    tags against context to prevent hallucinations. The core objective is to enable
    effective classification of text snippets with minimal hallucination risk by constraining
    the language model through validation contexts. The tutorial demonstrates creating
    request and response models, parallelizing classification tasks with asyncio.gather,
    and integrating the system into a FastAPI endpoint. Key concepts include asynchronous
    classification, schema validation, multi-class tagging, confidence scores, and
    production deployment considerations. Key phrases: user-defined tags, text classification,
    fastapi, pydantic, openai, async processing, parallel classification, schema validation,
    confidence scoring, API integration.'
examples/classification.md:
  hash: 57decdbd4602526cddd9ec86afe37f67
  summary: 'This tutorial explains how to implement single-label and multi-label text
    classification using OpenAI''s GPT models and Pydantic data models in Python.
    It emphasizes the use of Literal types over Enums for improved type safety, includes
    examples of defining response schemas with few-shot prompting techniques, and
    demonstrates classification functions with testing. Key concepts include chain-of-thought
    prompting, handling support ticket categorization, spam detection, and ensuring
    accurate multi-label predictions for NLP applications. Keywords: text classification,
    OpenAI API, GPT models, Pydantic, single-label, multi-label, Python, few-shot
    prompting, chain of thought, NLP, spam detection, support ticket categorization.'
examples/document_segmentation.md:
  hash: 121491f63507430563385c90fc98a84f
  summary: 'This comprehensive guide explores document segmentation using Large Language
    Models (LLMs), particularly Cohere''s command-r-plus model with 128k context length.
    It demonstrates how to organize long, complex texts into meaningful sections centered
    around key concepts by leveraging structured data classes (`Section`, `StructuredDocument`)
    and line numbering preprocessing. The approach enhances understanding of lengthy
    articles, such as tutorials on Transformer architectures, by extracting sections
    with specific topics. Key techniques include using LLMs for segmentation via system
    prompts, and reconstructing section texts based on start and end line indices.
    This method is applicable across domains for breaking down complex documents,
    code snippets, and mathematical content, improving content comprehension, summarization,
    and indexing. Keywords: document segmentation, Large Language Models, Cohere,
    Transformer, structured output, NLP, long documents, LLM-based text splitting,
    AI text organization.'
examples/entity_resolution.md:
  hash: b3f456d3d8db72c6526db22f548acca3
  summary: This guide explains how to extract, resolve, and visualize entities from
    legal documents and contracts using AI and graph visualization tools. It details
    the data structures for representing entities and their properties, methods for
    utilizing OpenAI's GPT-4 to automate entity extraction and resolution, and techniques
    for creating interactive entity graphs with Graphviz. Key topics include legal
    document analysis, entity resolution, dependency mapping, legal tech applications,
    and data visualization. This approach enhances understanding of complex legal
    contracts by highlighting interconnected clauses, obligations, and key terms for
    improved legal analysis and workflow efficiency.
examples/exact_citations.md:
  hash: 48305d437d19da6057fa731b883224c5
  summary: This document demonstrates how to validate AI-generated answers with contextual
    citations in Python using pydantic classes. It provides a framework to ensure
    answers are backed by accurate quotes from given text, preventing hallucinations.
    Key components include the `Fact` and `QuestionAnswer` classes, which validate
    sources and filter facts without valid citations. The `ask_ai` function integrates
    OpenAI's API with validation context, enabling reliable, citation-backed responses.
    Essential keywords include AI answer validation, Python, pydantic, citation validation,
    hallucination prevention, OpenAI, answer validation, context-based fact checking,
    and model validation.
examples/examples.md:
  hash: 44560a6b059cd1c58184b4e7fccc0bb4
  summary: This article explains how to incorporate examples into Pydantic models
    using the `json_schema_extra` parameter. By embedding practical examples within
    the model's schema, developers can enhance clarity and usability, especially for
    JSON schema generation and API documentation. The provided example demonstrates
    adding illustrative question-answer pairs to a `SyntheticQA` model, showcasing
    how to improve model documentation and facilitate synthetic data generation with
    OpenAI's GPT models. Keywords include Pydantic, JSON schema, model examples, data
    validation, API documentation, synthetic data, OpenAI, and schema customization.
examples/extract_contact_info.md:
  hash: 7a678fb17f5c490628a5f68d70bd67c9
  summary: This guide demonstrates how to automate customer lead information extraction
    using OpenAI's API and Pydantic for data validation. It focuses on modeling lead
    data with validated attributes like name and phone number, including handling
    phone number formats with country codes. The tutorial covers creating a function
    to extract multiple leads from user messages, ensuring accurate data collection
    for applications like chatbots. Key concepts include OpenAI integration, Pydantic
    data modeling, phone number validation, and automated lead extraction to streamline
    customer data management.
examples/extract_slides.md:
  hash: b55973085211e62a790101c2ab3a2a38
  summary: 'This guide demonstrates how to use AI to extract competitor data from
    presentation slides and images. It emphasizes the importance of collecting structured
    information, including text and embedded images, to identify industry-specific
    competitors and their features. The content provides a detailed data model using
    Python and Pydantic, and offers a practical implementation for analyzing slide
    images through AI-powered image recognition and natural language processing. Key
    concepts include data extraction, AI-driven competitor analysis, slide data processing,
    and automation of competitive intelligence gathering, making it ideal for businesses
    seeking efficient market and competitor research. Keywords: AI data extraction,
    competitor analysis, presentation slides, image recognition, structured data,
    market research, automation, Python, Pydantic.'
examples/extracting_receipts.md:
  hash: 1ce877006d4831a5eeeb0b64fb943fd0
  summary: This guide demonstrates how to use Python and GPT-4, combined with Pydantic
    for data validation, to extract and validate receipt data from images for automated
    expense tracking. It covers defining structured models for items and receipts,
    implementing custom validation to ensure total amounts match itemized sums, and
    utilizing the OpenAI GPT-4 API through the Instructor library for image analysis.
    Practical examples illustrate extracting receipt details from images, enabling
    efficient financial data processing and expense management. Keywords include GPT-4,
    Python, Pydantic, receipt data extraction, expense tracking, image analysis, data
    validation, OpenAI, automation.
examples/extracting_tables.md:
  hash: f7e39386e65d144db40b0549fc836164
  summary: This article demonstrates how to extract and convert tables from images
    into Markdown format using Python and OpenAI's GPT-Vision model. It covers building
    custom data types with Pydantic for handling Markdown tables, defining a Table
    class, and utilizing instructor's patched OpenAI client for image-based table
    extraction. Practical examples include extracting top-grossing app data from images,
    facilitating data analysis and automation. Key topics include GPT-Vision, Python
    data processing, image-to-table conversion, Markdown serialization, and leveraging
    AI for automated data extraction from images.
examples/groq.md:
  hash: 680f259ac1258ea7fe4eb11dc80babbf
  summary: 'Learn how to perform inference using Groq with the mixtral-8x7b model,
    including setup instructions, API key acquisition from GroqCloud, and practical
    Python examples. The guide covers package installations, environment variable
    configuration, and integrating Groq with the instructor library for seamless chat
    completions. Key topics include deploying Groq for AI inference, using the from_groq
    method, and creating structured JSON outputs, making it ideal for developers seeking
    efficient AI deployment solutions with Groq''s hardware and API. Keywords: Groq
    inference, AI deployment, mixtral-8x7b model, GroqCloud API, Python example, structured
    output, chat completions, AI inference setup.'
examples/image_to_ad_copy.md:
  hash: 70f33d5dd56c606567dafe15c58c5316
  summary: This content demonstrates how to leverage GPT-4 Vision API and ChatGPT
    to automatically generate advertising copy from product images, ideal for e-commerce,
    marketing, and retail teams. It details the process of identifying products within
    images, extracting key features and descriptions using AI models, and creating
    engaging ad headlines and persuasive marketing messages. The approach includes
    defining structured data models for products, error handling, and generating compelling
    ad copy tailored to each product. Key features include dynamic product attribute
    extraction, integration with OpenAI's vision models, and automated ad content
    creation to enhance online marketing efficiency and boost sales potential through
    effective visual-to-text conversion and advertising automation.
examples/index.md:
  hash: 260e691fbc028547afdea7dfe29cccfe
  summary: The Instructor Cookbook Collection offers practical examples and recipes
    for solving real-world problems using structured outputs across various domains,
    including text processing, multi-modal media, data tools, and deployment options.
    It features comprehensive guides on text classification, information extraction,
    document processing, vision processing, database integration, streaming, API integration,
    observability, and deployment with model providers like Groq, Mistral, IBM watsonx.ai,
    and Ollama. Designed to assist developers and AI practitioners, these cookbooks
    provide complete code, explanations, and best practices for implementing AI solutions
    effectively in production environments. Key keywords include AI recipes, structured
    outputs, text processing, multi-modal AI, data integration, deployment, model
    APIs, and open-source models.
examples/knowledge_graph.md:
  hash: 1a9bafb73950d7297949d435080373a4
  summary: This guide demonstrates how to create, visualize, and iteratively update
    knowledge graphs using Python, OpenAI's API, Pydantic, and Graphviz. It covers
    defining data structures with Node and Edge models, generating detailed knowledge
    graphs from complex topics like quantum mechanics, and visualizing these graphs
    with Graphviz. Key techniques include extracting key concepts and relationships
    with GPT-4, updating graphs step-by-step, and deduplicating nodes and edges for
    clarity. The tutorial emphasizes leveraging the Instructor library for structured
    outputs and iterative graph building, making it ideal for understanding complex
    subjects through visualizations. Core keywords include knowledge graphs, Python,
    OpenAI API, Pydantic, Graphviz, data visualization, AI, GPT-4, iterative updates,
    complex topics, and structured data modeling.
examples/local_classification.md:
  hash: c0f945e2d931625f632d70b4bfd3c92c
  summary: This article explains how to securely classify and handle confidential
    data using local AI models with llama-cpp-python and instructor, ensuring data
    privacy and infrastructure control. It covers setup instructions for installing
    models like Mistral-7B-Instruct-v0.2-GGUF, including GPU and CPU configurations,
    along with example Python code for processing confidential document queries such
    as content analysis, access permissions, and document metadata. The guide emphasizes
    maintaining data security by performing inference locally, making it ideal for
    organizations seeking secure AI solutions for sensitive information. Key keywords
    include local AI models, confidential data classification, llama-cpp-python, instructor,
    privacy-focused AI, and secure document handling.
examples/mistral.md:
  hash: 3d558fc653f72627f01a0c3de1f5a75a
  summary: This guide explains how to use MistralAI models for structured output inference
    with the `from_mistral` method. It covers obtaining a Mistral API key, setting
    up the environment with required packages, and provides an example of generating
    structured responses using the `Mistral` client and `instructor`. Key features
    include model selection (`mistral-large-latest`), mode configuration (`MISTRAL_TOOLS`),
    and defining response schemas with `pydantic`. Essential keywords include MistralAI,
    structured outputs, inference, API key, `from_mistral`, `Mistral` client, API
    setup, and example code.
examples/moderation.md:
  hash: c0d290b445a8b1d1076bc82a9fd8b361
  summary: "This document provides an example of utilizing OpenAI's moderation endpoint\
    \ to ensure content compliance with usage policies by filtering harmful content.\
    \ It explains how to implement an `AfterValidator` to automatically assess messages\
    \ for categories like hate, harassment, self-harm, sexual content, and violence.\
    \ The example includes code snippets demonstrating how to set up the moderation\
    \ validation with OpenAI\u2019s API, highlighting its ability to flag and reject\
    \ harmful or policy-violating messages. Key concepts include OpenAI moderation,\
    \ content filtering, safety validation, Pydantic integration, and ensuring API\
    \ input/output compliance for safe AI interactions."
examples/multi_modal_gemini.md:
  hash: d2d5cffd4469c75c6730fa3f130fecd1
  summary: 'This guide explains how to utilize Gemini with Google Generative AI for
    multi-modal data processing, specifically focusing on audio files. It details
    three methods: uploading entire audio files as normal messages, passing audio
    segments inline after installing pydub, and using lists of mixed content for flexible
    processing. The instructions emphasize setting the correct mode (GEMINI_JSON),
    uploading files with genai.upload_file, and providing audio data either as file
    objects or inline audio segments. These approaches enable efficient summarization,
    transcription, and analysis of audio recordings, supporting SEO by extracting
    core ideas, objectives, key details, and relevant keywords related to audio content
    processing with Gemini and Generative AI.'
examples/multiple_classification.md:
  hash: d80a59dabf71466f2ed5bc4178dc557b
  summary: This guide demonstrates how to implement multi-label classification for
    support ticket categorization using OpenAI's API and Pydantic. It introduces a
    custom enum and a Pydantic model to handle multiple labels such as "ACCOUNT,"
    "BILLING," and "GENERAL_QUERY," enabling effective multi-label predictions. The
    example illustrates how to set up the classification process with a tailored prompt
    and retrieve labels indicating multiple relevant categories for a given support
    ticket. Keywords include multi-label classification, OpenAI API, Pydantic, support
    ticket categorization, multi-label prediction, GPT-4, and effective support workflows.
examples/ollama.md:
  hash: 56fe05f28e384bbef8372e921efa4648
  summary: "This article explains how to utilize Ollama's local LLM server with the\
    \ Instructor library to generate structured outputs using Pydantic models. It\
    \ highlights the benefits of Instructor, such as a simple API, validation, reasking,\
    \ streaming support, and prompt control, enabling more precise and reliable AI\
    \ interactions. The guide provides practical steps and code examples for integrating\
    \ Ollama models like Llama 3 with Instructor\u2019s JSON schema validation, making\
    \ it easier to extract structured data from large language models for AI applications\
    \ and development."
examples/open_source.md:
  hash: 73e71b2d1f8dfeb4c1d6cfd3615ce795
  summary: This guide highlights open source model providers compatible with the OpenAI
    chat API, including OpenRouter, Perplexity, and RunPod LLMs. It demonstrates how
    to integrate these models with the OpenAI API chat endpoint and provides example
    resources for implementation. The setup utilizes text-generation-webui with an
    OpenAI plugin, enabling developers to leverage open source models like TheBloke
    LLMs for chat applications. Keywords include open source models, OpenAI chat API,
    OpenRouter, Perplexity, RunPod, text-generation-webui, and LLMs integration.
examples/pandas_df.md:
  hash: d08c46a6a8d4445ab9bf656ba28f6247
  summary: This guide demonstrates how to extract and convert Markdown tables directly
    into Pandas DataFrames in Python. It features techniques for parsing Markdown
    data, validating the DataFrame structure, and serializing it back to Markdown
    format using Pydantic annotations. The code showcases creating functions to extract
    tables with OpenAI's GPT-3.5-turbo model, enabling efficient data extraction from
    formatted Markdown tables. Key concepts include Markdown to DataFrame conversion,
    custom annotations for validation and serialization, and extracting structured
    data like tables with titles. Keywords include Pandas, Markdown parsing, data
    extraction, GPT-3.5-turbo, Python, DataFrame, table extraction, Pydantic, and
    OpenAI.
examples/partial_streaming.md:
  hash: b4fa99932aca3dffc93d4dea2b69e036
  summary: This article explains how to implement field-level streaming with the Instructor
    library in Python for dynamic UI rendering. It demonstrates using `Partial[T]`
    to create incremental, partial snapshots of model responses, enabling real-time
    updates. The example showcases extracting meeting and participant information
    from a text block using OpenAI's GPT-4, with streaming responses displayed via
    the Rich library. Key concepts include partial responses, stream processing, dynamic
    UI updates, and leveraging Instructor for field-level data handling in Python.
examples/pii.md:
  hash: 6cb6a88f6b787857b8da7d9a072b8cab
  summary: This guide demonstrates how to extract and scrub Personally Identifiable
    Information (PII) from documents using OpenAI's ChatCompletion model and Python.
    It covers defining Pydantic data models to structure PII data, utilizing OpenAI's
    API to extract sensitive information such as names, emails, phone numbers, addresses,
    and SSNs, and implementing a method to scrub PII by replacing values with placeholders.
    Key features include leveraging AI for accurate PII detection, data sanitization
    techniques, and customizable scrubbing methods to ensure privacy compliance in
    document processing workflows. Suitable keywords include PII extraction, data
    scrubbing, privacy, OpenAI, Python, AI-powered data anonymization, sensitive data
    protection, and document privacy.
examples/planning-tasks.md:
  hash: 00bfdb223b5c59a4fcafe1e6e020cfe8
  summary: This guide explains how to use OpenAI's Function Call ChatCompletion API
    for query planning in complex question-answering systems. It demonstrates how
    to define structured query models with Pydantic, create a query planner that breaks
    down a main question into dependent sub-questions, and leverages system prompts
    to generate organized query plans. The approach facilitates systematic information
    gathering, iterative querying, workflow automation, and process optimization,
    making it ideal for handling multi-step queries and knowledge graph extraction.
    Key concepts include structured schema design, dependency management, and leveraging
    OpenAI's models for automated query decomposition.
examples/recursive.md:
  hash: 32eb7db1d5fc4dc8fa262770848b0592
  summary: This guide explains how to implement recursive schemas using Pydantic models
    in Instructor, enabling the handling of hierarchical and nested data structures
    such as organizational charts, file systems, comment threads, and task dependencies.
    It covers defining recursive models, best practices like calling `model_rebuild()`,
    validation techniques for limiting recursion depth, and performance tips for managing
    complex data. The content emphasizes the importance of clear structure, validation,
    and practical examples to effectively work with recursive schemas in AI-powered
    applications.
examples/search.md:
  hash: 86f8d684546f51c59453bfcfcdf256cc
  summary: This article demonstrates how to segment search queries into actionable
    tasks using OpenAI Function Call and Pydantic. It showcases defining data structures
    with Pydantic, leveraging OpenAI's multi-task capabilities to split complex queries
    into multiple sub-queries, and executing them concurrently with asyncio. The example
    emphasizes extracting tasks like web searches, images, and videos from user input
    to improve virtual assistant functionality. Key concepts include OpenAI Function
    Call, Pydantic models, query segmentation, parallel execution, and applications
    in virtual assistants and search optimization.
examples/self_critique.md:
  hash: 15eeaa0bb27f7fc4c235f752faee8823
  summary: This guide explains how to implement self-correction in NLP applications
    using `llm_validator` for enhanced response accuracy. It demonstrates integrating
    validation callbacks within pydantic models to catch objectionable content, provide
    helpful error messages, and enable automatic retries with corrections. Key concepts
    include the use of `response_model`, custom validation with `llm_validator`, and
    retry mechanisms for self-healing language model outputs, making it a valuable
    resource for improving NLP model safety, reliability, and quality control. Keywords
    include self-correction, NLP validation, `llm_validator`, pydantic validation,
    self-healing AI, response accuracy, and prompt engineering.
examples/single_classification.md:
  hash: e57ed79f3f4234a0606723bb8c07d2ee
  summary: 'This guide demonstrates how to perform single-label text classification
    using the OpenAI API, specifically with the GPT-3.5-turbo and GPT-4 models. It
    showcases how to classify text as "SPAM" or "NOT_SPAM" with a response model,
    leveraging the instructor library for enhanced functionality. The example includes
    code for setting up the classification function, defining the response schema
    with Pydantic, and verifying predictions through sample inputs. Key features include
    the use of response_model for structured outputs, and the approach emphasizes
    simplicity and accuracy in spam detection and text classification tasks. Keywords:
    OpenAI API, single-label classification, GPT-3.5-turbo, GPT-4, text classification,
    spam detection, machine learning, natural language processing.'
examples/sqlmodel.md:
  hash: 481ac4bfa72d26340dd03cb9875960a6
  summary: This guide demonstrates how to seamlessly integrate SQLModel with Instructor
    AI in Python for efficient database and API development. It covers defining models
    that inherit from both SQLModel and Instructor's OpenAISchema, emphasizing the
    use of SkipJsonSchema to exclude fields like UUIDs from LLM prompts, ensuring
    accurate data generation. The tutorial provides comprehensive examples of creating,
    inserting, and managing database records using OpenAI's GPT models, highlighting
    best practices for model design, token optimization, and maintaining data consistency.
    Core keywords include SQLModel, Instructor AI, OpenAI, Python, database integration,
    automated record creation, UUID handling, and seamless API development.
examples/tables_from_vision.md:
  hash: 02f100035905072561af66bed755ecf7
  summary: This guide explains how to extract and convert tables from images into
    markdown format using OpenAI's GPT-4 Vision model. It details the process of analyzing
    images to identify table headers, generate descriptive titles and summaries, and
    output structured markdown tables with captions. The method leverages Python,
    pandas, and pydantic for data handling, emphasizing automatic data extraction,
    table serialization, and effective data presentation from visual content. Key
    concepts include image analysis, data extraction, markdown formatting, and GPT-4's
    powerful vision capabilities for accurate table conversion.
examples/tracing_with_langfuse.md:
  hash: 2b1caa40e9da271b66e341c45b463b28
  summary: This guide introduces Langfuse, an open-source observability and tracing
    platform for AI applications, showcasing how to integrate it with Instructor and
    OpenAI clients for enhanced monitoring and debugging of large language model (LLM)
    calls. It provides setup instructions, including installation and environment
    configuration for both synchronous and asynchronous OpenAI clients. The content
    highlights key use cases such as tracing API calls, classifying customer feedback,
    scoring relevance, and visualizing detailed traces via the Langfuse dashboard.
    Core keywords include Langfuse, observability, AI monitoring, tracing, LLM, API
    performance, debugging, Instructor, OpenAI, and asynchronous AI integration.
examples/watsonx.md:
  hash: dafd5f18905aa8c25b71a9f2f9bc8a65
  summary: This guide details how to use IBM watsonx.ai for inference with LiteLLM
    to generate structured outputs. It covers prerequisites such as IBM Cloud account,
    API key, and project ID, and provides installation instructions using Poetry.
    The example demonstrates creating a custom data model and performing JSON-mode
    inference with watsonx.ai, showcasing how to set environment variables, initialize
    the client, and generate structured data like company information from text input.
    Key concepts include IBM watsonx.ai, LiteLLM, inference, structured outputs, setup,
    API integration, and Python coding examples.
examples/youtube_clips.md:
  hash: 972f468e337dd6fc72cfc12cbd129226
  summary: This guide explains how to generate concise, engaging YouTube clips from
    video transcripts using the `instructor` library and OpenAI models. It demonstrates
    extracting transcript segments with timing information from YouTube videos using
    `youtube_transcript_api`, and then leveraging GPT-4 to identify key moments and
    create specific clip titles and descriptions. The process involves fetching transcripts,
    prompting GPT-4 to produce notable clips, and displaying the results in a structured
    format. Key concepts include transcript extraction, AI-powered clip generation,
    content summarization, and leveraging OpenAI for enhanced video editing and content
    segmentation. This approach helps content creators enhance engagement by recutting
    videos into focused, shareable clips.
faq.md:
  hash: bca382d72ff309ba7f12a9213923c7e5
  summary: Instructor is a versatile Python library designed to simplify extracting
    structured data from Large Language Models (LLMs) by leveraging Pydantic schemas
    for validation and consistency across various providers like OpenAI, Anthropic,
    Google Gemini, Cohere, and open-source models. It offers multiple modes, such
    as JSON, Tools, and Function Calling, to suit different provider capabilities,
    along with features like response validation, automatic retries, raw response
    access, and streaming support. Ideal for integrating LLMs into applications, Instructor
    also supports fastapi compatibility, async operations, and cost optimization through
    prompt design and caching. Core keywords include LLM, Pydantic, structured data,
    AI integration, OpenAI, Anthropic, Google Gemini, function calling, retries, streaming,
    API, and chat models.
getting-started.md:
  hash: a9ee178a3479ee94b3224d8bb4aeb121
  summary: Getting Started with Instructor provides a comprehensive guide to extracting
    structured data from large language models (LLMs) using Python. It covers installation
    of the Instructor library for various AI providers like OpenAI, Anthropic, Google,
    Cohere, and more, along with environment setup and API key configuration. The
    guide demonstrates defining Pydantic models for output validation, handling validation
    errors, working with complex nested data structures, and utilizing streaming responses
    for improved user experience. Key features include seamless integration with different
    LLM providers, validation and error handling, support for streaming, and flexible
    configuration for structured output extraction. This tutorial is ideal for developers
    seeking to automate and validate structured data extraction from conversational
    AI models in Python.
help.md:
  hash: 8aa79aef3783bdc81724f7d3d6d1b7d1
  summary: This guide provides essential resources for getting help with Instructor,
    an AI model prompting tool. Key support options include the Discord community,
    detailed concepts on prompting, practical cookbooks with usage examples, and informative
    blog articles. Additionally, users can leverage GitHub Discussions for questions
    and collaboration, report bugs and request features via GitHub Issues, or contact
    the creator on Twitter. These resources ensure users can effectively learn, troubleshoot,
    and optimize their experience with Instructor.
index.md:
  hash: 69eabac780a7f8433fa284fdf6804bc6
  summary: 'Instructor is a popular open-source Python library that enables structured
    data extraction from Large Language Models (LLMs) like OpenAI, Anthropic, Google,
    Ollama, DeepSeek, and others. Built on Pydantic, it provides type-safe validation,
    automatic retries, streaming support, and a unified API for multiple providers,
    facilitating reliable and validated LLM outputs. Key features include defining
    schemas for precise data extraction, multi-provider flexibility, support for open-source
    models, and seamless integration with async Python. Instructor is ideal for applications
    requiring high-quality, validated AI responses, making it a robust solution for
    LLM data validation, prompting, and production deployment. Key words include:
    LLM structured outputs, Python, validation, Pydantic, OpenAI, Anthropic, Google,
    Ollama, DeepSeek, streaming, retries, type safety, open source, multi-provider.'
installation.md:
  hash: a6fe720590b602e1f753c067be9c3121
  summary: Learn how to install Instructor, an advanced Python tool for building CLIs,
    using pip. Instructor requires dependencies such as openai, typer, docstring-parser,
    and pydantic, making setup straightforward for Python 3.9 and above. This guide
    provides a simple, quick installation process to enhance your Python projects
    with powerful, type-hint-based CLI development.
integrations/anthropic.md:
  hash: 4dc4a8447b0ba347dec649df41270398
  summary: 'The guide "Structured Outputs with Anthropic, a Complete Guide w/ Instructor"
    provides a comprehensive tutorial on integrating the Anthropic and Instructor
    clients in Python to create user models with complex properties. It covers installing
    the Instructor client with Anthropic support, using the `from_anthropic` method,
    and applying structured data extraction through the `instructor.Mode.ANTHROPIC_TOOLS`
    API. The guide includes examples for multimodal inputs like images and PDFs, streaming
    responses, caching, and extended thinking with Anthropic''s latest models. Key
    features include the use of Pydantic models for data validation, handling multimodal
    inputs consistently, streaming support with iterables and partial objects, and
    caching for efficient repeated use.


    Keywords: Anthropic, Instructor client, Python, structured outputs, multimodal
    inputs, Pydantic models, caching, streaming support, Anthropic tools, Claude models,
    data extraction, API integration, SEO.'
integrations/anyscale.md:
  hash: 53e83cd7c07b43d303cb4a8696300408
  summary: This guide provides instructions on using Anyscale, a platform offering
    access to open-source LLMs like Mistral and Llama models, with the Instructor
    library to produce structured outputs. It covers installation, API key setup,
    and offers a practical example of extracting structured data using Anyscale's
    API and the Instructor client in JSON schema mode. Supported modes include JSON,
    JSON_SCHEMA, TOOLS, and MD_JSON, and the platform features a variety of models
    such as Mistral and Llama, making it a comprehensive resource for leveraging open-source
    LLMs for structured data extraction and AI development.
integrations/azure.md:
  hash: 3a23c67e1ceafad28834395d384f37ff
  summary: This comprehensive guide explains how to use Azure OpenAI with Instructor
    for structured outputs, including synchronous and asynchronous implementations,
    streaming, nested models, and response validation. It covers installation, authentication,
    deploying models, and working with various response modes such as JSON, tools,
    and function calling. Key features include streaming partial and iterable responses,
    handling complex nested data, and leveraging different Instructor modes to optimize
    structured output generation. This resource is ideal for developers seeking secure,
    enterprise-grade AI solutions with Azure OpenAI and Instructor for reliable, scalable
    structured data extraction.
integrations/bedrock.md:
  hash: 52ede618fbd9c3a9edc9355537e1eb51
  summary: This guide explains how to use AWS Bedrock with Instructor and Pydantic
    for generating structured, validated JSON outputs from Amazon's foundational AI
    models. It covers setting up the AWS Bedrock client, implementing type-safe responses
    with Pydantic models, and utilizing different modes like BEDROCK_TOOLS and BEDROCK_JSON
    for flexible output formats. The tutorial also demonstrates handling nested objects
    and complex data structures, enabling developers to create robust, structured
    AI interactions in Python. Core keywords include AWS Bedrock, Instructor, Pydantic,
    JSON outputs, structured responses, AI models, and type safety.
integrations/cerebras.md:
  hash: 30881d913bf857193a0b5af812d259c2
  summary: This comprehensive guide details how to use Instructor with Cerebras's
    hardware-accelerated AI models for generating structured, type-safe outputs. It
    covers installation, both synchronous and asynchronous usage examples, and advanced
    features like nested outputs and streaming support, including partial and iterable
    streaming modes. The guide highlights customization through Instructor hooks and
    explains different response modes such as CEREBRAS_JSON and CEREBRAS_TOOLS, emphasizing
    the flexibility and future-proofing of these modes for high-performance, validated
    AI responses. Key terms include Cerebras, Instructor, structured outputs, JSON
    parsing, streaming, validation hooks, and AI model integration.
integrations/cohere.md:
  hash: bcabf6169d2e18732d09f41a2b03ee9a
  summary: This guide provides a comprehensive tutorial on generating structured,
    type-safe outputs with Cohere's command models using the Instructor library in
    Python. It covers setup instructions, including installing the library and obtaining
    an API key. The tutorial demonstrates how to define data models with Pydantic,
    patch the Cohere client with Instructor for enhanced capabilities, and generate
    structured responses such as creating a detailed Group object based on provided
    text. Key features include leveraging Cohere's command models like "command-r-plus"
    to produce accurate, JSON-formatted data, making it ideal for tasks requiring
    structured outputs, data extraction, and automation. This resource is valuable
    for developers seeking to enhance NLP workflows with reliable, structured data
    generation.
integrations/cortex.md:
  hash: 5dc3985ba626ba07487689f654305962
  summary: This guide provides a comprehensive overview of using Cortex with Instructor
    to achieve structured outputs from local open-source large language models (LLMs).
    It covers quick setup, both synchronous and asynchronous API usage, and demonstrates
    advanced nested extraction examples with Pydantic models. Key topics include model
    deployment with Cortex, integration with OpenAI clients, and effective prompt
    handling for structured data extraction. Essential keywords include Cortex, Instructor,
    LLM, structured outputs, local models, open-source, API integration, Pydantic,
    and AI prompt engineering.
integrations/databricks.md:
  hash: 10a70b86eb06ad1262a58d8050984151
  summary: This guide provides a comprehensive overview of using Databricks with the
    Instructor library to obtain structured outputs from AI models. It covers installation,
    setting up environment variables with Databricks API keys and workspace URL, and
    demonstrates a basic example of extracting structured data such as user information
    using Databricks models. The guide highlights supported modes like TOOLS, JSON,
    FUNCTIONS, and more, and explains that Databricks offers access to various models,
    including foundation, fine-tuned, and open-source models deployed on the platform.
    Keywords include Databricks, Instructor, structured outputs, AI models, API integration,
    and machine learning.
integrations/deepseek.md:
  hash: 8e0bf42ff9f31e84527488ce3b43e8d9
  summary: This guide provides a comprehensive overview of using DeepSeek models with
    Instructor for type-safe, structured outputs. DeepSeek, a Chinese AI company,
    offers various models including the deepseek coder, chat model, and R1 reasoning
    model. The tutorial demonstrates how to set up and utilize models for both synchronous
    and asynchronous scenarios using the OpenAI API. Key features include creating
    structured outputs with Pydantic, streaming with iterables and partials, and integrating
    reasoning models for detailed completion traces. Essential steps for setting up
    include initializing the `instructor` package, configuring the API key, and using
    the appropriate Instructor modes. Core keywords include DeepSeek, AI models, structured
    outputs, type-safe, OpenAI API, Instructor, Pydantic, synchronous, asynchronous,
    and reasoning models.
integrations/fireworks.md:
  hash: 542aa4056ddd0ae3132abdbd10cbffa2
  summary: This comprehensive guide provides instructions on utilizing Instructor
    with Fireworks AI models to create structured, type-safe outputs. It covers installation,
    basic synchronous and asynchronous user examples, and complex nested examples,
    emphasizing high-performance and cost-effective AI capabilities. The guide also
    demonstrates streaming support, including iterables and partial streaming, using
    Pydantic models for type validation. Key points include integration with `Fireworks`,
    usage of `instructor` modes for structured outputs, and maintaining compatibility
    with the latest Fireworks API versions. Essential keywords include Fireworks AI,
    Instructor, structured outputs, type-safe, streaming support, and Pydantic.
integrations/genai.md:
  hash: 3757d0135f6603b7a91ddc2647869dcb
  summary: This guide explains how to use Google's Generative AI SDK (`genai`) with
    Instructor to extract structured data from Gemini models. It covers key features
    such as two modes (`GENAI_TOOLS` and `GENAI_STRUCTURED_OUTPUTS`), message formatting
    options, system messages, template variables, validation with retries, and multimodal
    capabilities including image, audio, and PDF processing. The article highlights
    seamless handling of files via URLs, local paths, or base64, along with support
    for streaming responses and async operations. Core keywords include Google genai
    SDK, Gemini models, structured outputs, multimodal AI, PDF, image, audio processing,
    validation, retries, streaming, async support, and data extraction.
integrations/google.md:
  hash: 4212d9c58eddfc09f40498f216d6e23d
  summary: 'This comprehensive guide explores how to utilize Instructor with Google''s
    Gemini models to generate structured, type-safe AI responses. It covers setup
    instructions, both synchronous and asynchronous usage, configuration options for
    fine-tuning output, and advanced features like nested responses, streaming, partials,
    and iterables. The guide also highlights Instructor''s support for Gemini''s multimodal
    capabilities, various response modes (JSON and tool calling), and compatibility
    with multiple Gemini models. Key topics include generating validated structured
    outputs, leveraging multimodal AI, configuration customization, and integrating
    Gemini''s powerful features for tasks like PDF parsing, video recommendations,
    and citations. Keywords: Google Gemini, Instructor, structured AI responses, multimodal
    AI, type-safe outputs, async support, streaming, configuration options, AI models,
    PDF parsing, multimodal capabilities.'
integrations/groq.md:
  hash: 1b2b59a31e2e4ce05dff63e482192a95
  summary: The article provides a detailed guide on using Groq AI with Pydantic to
    generate structured outputs in Python. It highlights using the `llama-3-groq-70b-8192-tool-use-preview`
    model to create type-safe, structured responses via synchronous and asynchronous
    examples. The guide emphasizes setting up with an API key, employing Groq's LLM
    models, and integrating Pydantic for defining response structures. It also demonstrates
    creating nested object responses for complex data extraction. Key terms include
    Groq AI, Pydantic, structured outputs, type-safe responses, and Python API integration.
integrations/index.md:
  hash: f60c24d97e6652fc638afbee2b2f5cb4
  summary: Instructor Provider Integrations enables seamless connection to a wide
    range of LLM providers including OpenAI, Anthropic, Google, AWS Bedrock, Cohere,
    Mistral, and open-source models like Ollama and llama-cpp-python. It offers core
    features such as model patching, response schema validation with Pydantic, response
    streaming, and hooks for monitoring. Supports multiple provider modes like tools,
    JSON, markdown-embedded JSON, and AWS Bedrock functions, facilitating flexible
    structured output generation. Users can initialize providers easily with simple
    strings or set up clients manually, ensuring versatile integration options. Key
    keywords include LLM, AI providers, structured outputs, Model Patching, JSON Response,
    Open Source, Cloud AI, and SDK integration.
integrations/litellm.md:
  hash: d6fc058af4b92fbded142d630ec90055
  summary: This comprehensive guide explains how to use Instructor with LiteLLM's
    unified interface to generate structured, type-safe outputs across multiple LLM
    providers like GPT-3.5 and Claude-3. It covers both synchronous and asynchronous
    implementations, demonstrating how to create validated responses using Pydantic
    models. Additionally, the guide details cost calculation via response cost attributes
    and emphasizes LiteLLM's compatibility and easy model switching. Key topics include
    structured output generation, response validation, cost tracking, and integration
    with various LLM providers.
integrations/llama-cpp-python.md:
  hash: d4baa4f29b79ed75acefbd1acaec8481
  summary: This comprehensive guide explores how to generate structured, type-safe
    outputs using llama-cpp-python with Instructor, focusing on JSON schema mode and
    speculative decoding. By leveraging open-source LLMs, users can achieve structured
    outputs with constrained sampling techniques and avoid network dependencies using
    an OpenAI-compatible client. The guide highlights features such as the `response_model`
    and `max_retries` for enhanced functionality in `create` calls, showcasing the
    use of Pydantic for efficient data validation. An advanced example using JSON
    schema to extract data within a streaming context is also presented. Key terms
    include llama-cpp-python, JSON schema mode, speculative decoding, Pydantic, and
    structured outputs.
integrations/mistral.md:
  hash: f821daf9ad84fd47d59dd265143b200b
  summary: This comprehensive guide explains how to use Mistral AI's Large model with
    Instructor to generate structured, type-safe outputs and JSON schema-based function
    calling. It covers setup instructions, including API key configuration, and showcases
    how to utilize Mistral's capabilities in both synchronous and asynchronous modes,
    with support for nested models, streaming, and multimodal PDF analysis. Key features
    include modes for structured outputs, partial response streaming, iterable responses,
    and advanced multimodal extraction, making it an essential resource for leveraging
    Mistral's powerful AI models with Instructor for reliable data extraction and
    structured AI responses.
integrations/ollama.md:
  hash: 3b4581a025f34d27c2ccb1cae6234478
  summary: This comprehensive guide covers how to generate structured, type-safe outputs
    using Ollama with Instructor, focusing on JSON schema mode and local LLMs. It
    explains key features like patching, response modeling with Pydantic, and timeout
    handling for reliable request management. The article offers setup instructions,
    including auto and manual configurations, mode selection based on model types,
    and best practices for timeout settings. Key topics include Ollama integration,
    open-source LLMs, JSON schema validation, function calling, and performance optimization,
    making it a valuable resource for developers seeking structured AI outputs and
    advanced prompt engineering.
integrations/openai-responses.md:
  hash: 97cf410cfb0e22b4ec38c284d7825976
  summary: "The OpenAI Responses API guide provides comprehensive instructions for\
    \ using Instructor\u2019s new API to achieve structured, type-safe outputs with\
    \ OpenAI models like GPT-4.1. It covers core functionalities such as creating\
    \ responses synchronously and asynchronously, utilizing response modes, and handling\
    \ iterable and partial outputs for dynamic workflows. The API also integrates\
    \ powerful built-in tools like web search and file search, enabling real-time\
    \ information retrieval and knowledge base enhancements. Key features include\
    \ response validation via Pydantic models, support for multiple tools, and examples\
    \ demonstrating both basic and advanced use cases, making it ideal for building\
    \ reliable AI-driven applications. Keywords include OpenAI Responses API, Instructor\
    \ API, structured outputs, web search, file search, GPT-4, synchronous/asynchronous\
    \ methods, type validation."
integrations/openai.md:
  hash: e590f98025395a6720663e19033615a5
  summary: This comprehensive guide explores using OpenAI models with Instructor for
    structured, type-safe outputs, including GPT-4, GPT-3.5, and multimodal capabilities
    with images, audio, and PDFs. It covers setup, both synchronous and asynchronous
    examples, nested data extraction, multimodal analysis, streaming, batching, and
    various response modes like tools and JSON modes. The tutorial emphasizes best
    practices for model selection, performance optimization, and common use cases
    such as data extraction, document analysis, form parsing, and API response structuring.
    Keywords include OpenAI, Instructor, structured outputs, GPT-4, multimodal, streaming,
    batch API, data extraction, type-safe responses, and API integrations.
integrations/openrouter.md:
  hash: bd8e8fdd749c0da0250180d12cc97e4e
  summary: 'This comprehensive guide explains how to use Instructor with OpenRouter
    to achieve structured, type-safe outputs across multiple large language model
    (LLM) providers. It details how to integrate Instructor with the OpenAI client,
    supporting synchronous and asynchronous usage, nested object extraction, and various
    modes including Structured Outputs and JSON. The guide emphasizes the importance
    of model compatibility with tool calling and structured outputs, provides code
    examples for different scenarios, and highlights how to enable streaming responses.
    Key topics include multi-provider API switching, schema validation with Pydantic
    models, handling models without tool calling support, and leveraging OpenRouter''s
    unified API for enhanced LLM integrations. Core keywords: OpenRouter, Instructor,
    LLM, structured outputs, tool calling, API integration, type-safe responses, multi-provider,
    GPT models, JSON mode, streaming.'
integrations/perplexity.md:
  hash: 415fc2e8761f4382e5ec5ebd37673a80
  summary: Learn how to generate structured, validated JSON outputs with Perplexity
    AI using Instructor and Pydantic models. This guide covers initializing Perplexity
    clients, enabling instructor patches, and creating type-safe responses for both
    synchronous and asynchronous use cases. It includes examples for handling nested
    objects and utilizing the Perplexity JSON mode, ensuring precise, structured data
    extraction from language models. Key topics include Perplexity API integration,
    mode support, and leveraging Pydantic for data validation.
integrations/sambanova.md:
  hash: 81003730e09b4b43bccdd04a11b7f3ae
  summary: SambaNova integration with Instructor allows users to leverage SambaNova's
    LLM API for structured output generation in Python. The setup involves installing
    the `instructor[openai]` package and configuring the client with the SambaNova
    API endpoint and API key. It supports both synchronous and asynchronous usage,
    enabling detailed prompt and response modeling with Pydantic. Key models include
    Meta-Llama-3.1-405B-Instruct, and users can explore additional options via SambaNova's
    documentation. This integration facilitates advanced AI workflows with SambaNova's
    large language models for enhanced NLP applications.
integrations/together.md:
  hash: 39d3ac703bab17e5ad0cb06d6c0cafd6
  summary: 'This comprehensive guide explains how to use Together AI with Instructor
    to generate structured, type-safe outputs through function calling. It highlights
    open-source LLM support, patching features like response models and retries, and
    demonstrates how to integrate Instructor with Together''s models using Python.
    Key topics include leveraging Pydantic for data validation, utilizing Together
    AI''s API, and creating custom models for accurate output extraction. Keywords:
    Together AI, Instructor, structured outputs, function calling, open-source LLMs,
    Python, Pydantic, type-safe responses, API integration.'
integrations/vertex.md:
  hash: 7ce4a77dd1a123fbb6476b811d4eb330
  summary: This comprehensive guide details how to leverage Google Cloud's Vertex
    AI with Instructor for creating structured, type-safe AI responses. It covers
    installation, quick start examples for synchronous and asynchronous usage, and
    advanced streaming features such as partial response streaming and iterable collections.
    Key topics include integrating Vertex AI generative models, validating outputs
    with Pydantic models, and utilizing both streaming and async capabilities for
    real-time AI interactions. Perfect for enterprise AI developers, this guide emphasizes
    scalability, security, and compatibility with the latest Vertex AI API versions.
integrations/writer.md:
  hash: 27299f8967d9a30443039b93e1d233dd
  summary: 'This guide provides a comprehensive overview of using Writer for structured
    outputs with the latest Palmyra-X-004 model, which enhances reliability using
    tool-calling functionality. It includes setup instructions, such as obtaining
    an API key and integrating with Python using Writer''s `instructor` module. The
    guide offers synchronous and asynchronous examples for extracting structured data,
    including support for nested objects and streaming responses with iterables and
    partial streaming. Key topics include structured data extraction, API integration,
    Python scripting, and advanced data handling with Writer''s Palmyra-X-004 model.
    Keywords: Writer, Palmyra-X-004, structured outputs, API key, data extraction,
    nested objects, streaming support, Python integration.'
jobs.md:
  hash: d41d8cd98f00b204e9800998ecf8427e
  summary: Of course! Please provide the text that you would like me to summarize,
    and I'll be happy to assist you.
learning/getting_started/client_setup.md:
  hash: 4fe10537bbb9d86a42491b3167294463
  summary: This guide provides an overview of setting up clients with various LLM
    providers using Instructor, including OpenAI, Anthropic, Google Gemini, Cohere,
    and Mistral. It explains how to configure different modes like TOOLS, JSON, and
    provider-specific modes such as ANTHROPIC_TOOLS and GEMINI_TOOLS, helping users
    select the best mode for their needs. Additionally, it covers asynchronous client
    configurations, advanced OpenAI settings, and utilizing OpenAI-compatible interfaces
    from providers like Azure OpenAI and Groq. Essential keywords include LLM providers,
    client setup, Instructor modes, OpenAI configuration, asynchronous operation,
    and OpenAI-compatible API.
learning/getting_started/first_extraction.md:
  hash: baef8e4190f7d8511867014cc30105a6
  summary: This guide provides a step-by-step tutorial on creating a structured data
    extraction using the OpenAI-powered Instructor library, focusing on extracting
    a person's name and age from text. It outlines how to define a data model using
    the Pydantic library, set up an Instructor client, and perform the extraction
    using a model like "gpt-3.5-turbo". The response is returned as a type-safe Python
    object. Additionally, the guide covers enhancing the extraction process with field
    descriptions and handling optional information. Key concepts include structured
    data extraction, LLMs, data modeling, and typed responses with Python.
learning/getting_started/installation.md:
  hash: 5b84a6ceca57668e3313ce05bf2cc7fc
  summary: The guide provides comprehensive instructions for installing and setting
    up the Instructor Python library, which is designed to work with various LLM (Large
    Language Model) providers to extract structured outputs. Key steps include basic
    installation with `pip install instructor` and setting up Pydantic for data models.
    It details specific steps for integrating with popular LLM providers like OpenAI,
    Anthropic (Claude), Google Gemini, Cohere, Mistral, and LiteLLM, including how
    to configure API keys for each. Users can verify their installation by running
    a simple data extraction example. The guide also directs users to further resources
    for client setup, enhancing their ability to work with different LLMs.
learning/getting_started/response_models.md:
  hash: 439f40be0a2357543b7386eaddada0c0
  summary: This guide on response models covers the core functionality of creating
    and validating data structures with Pydantic in the Instructor framework. It details
    how to define basic models, add field metadata, and implement validation rules
    to ensure data integrity. Key topics include field validation, nested and optional
    fields, enums, handling lists, and using models with the Instructor and OpenAI's
    GPT to extract structured data. Advanced validation methods and the importance
    of model documentation are also highlighted, providing insights essential for
    developers working with AI-driven data extraction and processing.
learning/getting_started/structured_outputs.md:
  hash: 7a88cbee6c536e3cbb5a0cc0d72a0679
  summary: 'The article discusses how to effectively extract structured data using
    large language models (LLMs) by employing structured outputs. It highlights the
    inconsistency challenge associated with unstructured outputs using LLMs like GPT-3.5-turbo,
    which can yield variable formats. To address this, structured outputs and Pydantic
    models are leveraged to ensure consistent, machine-readable data extraction, enabling
    validation, type safety, and self-documentation. Key examples include using Pydantic
    models with fields such as `name`, `age`, and `email`, as well as handling complex
    nested structures like `Address`, `Contact`, and `Person`. The use of the Instructor
    tool is emphasized for setting up structured outputs, and installation steps via
    pip are provided alongside instructions for setting up LLM provider API keys.
    Keywords: structured data extraction, large language models, Pydantic, machine-readable
    formats, LLMs, validation, type safety, Instructor tool.'
learning/index.md:
  hash: cef062227d3d860ba9c5429902c3b19e
  summary: "This guide provides comprehensive examples and tutorials for using Instructor\
    \ to extract structured data from language models. It covers essential topics\
    \ such as installation, creating first extraction models, understanding response\
    \ types, and setting up clients for different providers. The resource also delves\
    \ into basic extraction patterns\u2014including simple objects, lists, nested\
    \ structures, optional fields, and validation techniques\u2014along with advanced\
    \ concepts like prompt templates, custom validators, and retry mechanisms. Additionally,\
    \ it features sections on validation best practices and streaming results, making\
    \ it a valuable resource for developers seeking to deploy efficient, accurate\
    \ language model data extraction solutions. Key keywords include structured data\
    \ extraction, Instructor, language models, validation, prompt templates, streaming,\
    \ and API setup."
learning/patterns/field_validation.md:
  hash: d83ec3b378bd8521ddca3ec910af0b09
  summary: This comprehensive guide on field validation with Instructor explains how
    to ensure data quality and enforce constraints using Pydantic validation features.
    It covers basic field constraints (like min/max length, regex patterns, and number
    ranges), advanced validation with field validators and model validators for complex
    logic, nested structure validation, list item validation, and enumeration-based
    validation. The guide also details customizing error messages, handling validation
    failures, and examples such as form data validation. It emphasizes the importance
    of validation for data consistency, business rule enforcement, and error prevention
    in structured data extraction. Key concepts include Pydantic, field constraints,
    validators, enums, nested validation, and retry mechanisms for validation errors.
learning/patterns/list_extraction.md:
  hash: 52df07e9829a814eab6099869f507f49
  summary: This comprehensive guide covers list extraction techniques using Instructor,
    focusing on how to extract structured lists, nested lists, and arrays from text
    with OpenAI models. It explains defining item models, direct list extraction,
    handling nested data, streaming list results, validating and constraining lists
    with Pydantic, and applying these methods to real-world scenarios like task and
    product data extraction. Key concepts include list models, validation, constraints,
    streaming, and nested structures, making it essential for developers integrating
    complex data extraction, automation, and data validation workflows with AI.
learning/patterns/nested_structure.md:
  hash: 31c4a779073ce7b3c4ae057566c29219
  summary: This guide provides comprehensive instructions for extracting and working
    with nested data structures using Instructor and Pydantic. It covers basic to
    complex hierarchies, including nested objects, lists, optional fields, validation,
    and recursive models. Key concepts include representing parent-child relationships,
    multi-level nesting, handling optional nested data, and validating nested fields
    to ensure data integrity. Practical examples demonstrate extracting organizational
    charts, employee profiles, recipes, and recursive comment threads, making it a
    valuable resource for managing hierarchical data extraction with structured validation
    in AI-powered applications.
learning/patterns/optional_fields.md:
  hash: 3575bc03921692c8eb0c6deb127304d1
  summary: This guide provides an overview of handling optional fields in data models
    using Python and Pydantic, emphasizing flexible data extraction when information
    is missing or uncertain. It explains how to define optional fields with default
    values, validation, and nested structures, as well as leveraging the instructor
    library's `Maybe` type for uncertain data. Key concepts include handling `None`
    values, validation techniques, and best practices for working with complex or
    incomplete data, making this resource essential for robust data modeling and extraction
    in AI applications. Core keywords include optional fields, data modeling, validation,
    nested structures, Maybe type, Pydantic, Python, instructor, and incomplete data
    handling.
learning/patterns/prompt_templates.md:
  hash: fb497612252f924c659fedf6429d075b
  summary: This guide explains how to use prompt templates with Instructor for creating
    reusable, parameterized prompts for structured data extraction. It covers basic
    prompt templating with strings and f-strings, as well as advanced techniques using
    template functions to handle complex tasks. Best practices include clear output
    formatting, consistent language, concise prompts, and testing with diverse inputs.
    The content emphasizes standardizing prompt design, separating prompt engineering
    from application logic, and optimizing prompts for accuracy and efficiency in
    data extraction. Keywords include prompt templates, structured data extraction,
    reusable prompts, template functions, best practices, and AI prompting techniques.
learning/patterns/simple_object.md:
  hash: 9e6f109db11b651d3454f1b2628b9f95
  summary: This guide explains simple object extraction from text using pydantic models
    and instructor with OpenAI's GPT-3.5-turbo. It covers defining structured data
    models with fields, descriptions, optional fields, and validation rules to accurately
    extract information such as personal details, book data, and contact info. The
    content emphasizes best practices for structured data extraction, handling missing
    fields, and applying validation for real-world applications. Key keywords include
    object extraction, data modeling, pydantic, instruction-based extraction, OpenAI,
    structured data, field descriptions, optional fields, and validation techniques.
learning/streaming/basics.md:
  hash: a4913c779ed02b6618fcdcc806adb6f3
  summary: Streaming allows for real-time, incremental reception of responses from
    AI models, enhancing user experience through faster perceived response times,
    progressive UI updates, and simultaneous data processing. It works by enabling
    streaming mode (`stream=True`), which returns an iterator of partial responses
    that update fields incrementally. Key concepts include checking for completed
    fields with `hasattr()`, tracking progress by monitoring which fields are received,
    and implementing progressive updates in applications. This approach optimizes
    interactions with models like GPT-3.5-turbo, supporting dynamic data handling
    and improved responsiveness in AI-powered interfaces.
learning/streaming/lists.md:
  hash: 5ba152a331d57506a331247c50f21b88
  summary: This guide explains how to stream lists of structured data using Instructor,
    a tool for processing collection items as they are generated to enhance responsiveness.
    It demonstrates defining Pydantic models for list items, specifying response types
    with Python's typing system, and processing streamed data in real-time. Key features
    include streaming list outputs, progress tracking during item generation, and
    practical examples such as generating tasks for projects. The guide also references
    related resources on streaming fundamentals, list extraction, validation, and
    API integration, making it valuable for developers working with real-time data
    processing and structured streaming in AI applications.
learning/validation/basics.md:
  hash: dceebe01a5105d306522e815b6f6cc6d
  summary: This guide on validation with Instructor explains how to ensure data quality
    from LLM outputs through structured validation. It covers core validation concepts
    like data integrity, consistency, and quality, utilizing tools such as Pydantic
    for type checking, required fields, minimum and maximum values, and custom error
    messages. The process involves validating generated responses, capturing errors,
    and retrying until validation passes. It also highlights essential validation
    types, integration with Instructor, and advanced topics like custom validators
    and retry mechanisms for maintaining high-quality, structured data from language
    models.
learning/validation/custom_validators.md:
  hash: 91620efed812513429dec2bb7d7e40f1
  summary: This tutorial on custom validators in Pydantic within the context of Instructor
    provides detailed guidance on implementing specialized validation logic for structured
    data extraction. It covers both rule-based and semantic validators, explaining
    how to apply Pydantic field validators, create complex validations for multiple
    fields, and handle complex data types like emails and phone numbers. Additionally,
    the guide introduces semantic validation using language models (LLMs) for subjective
    and contextual criteria, highlighting when to use such methods. The tutorial also
    addresses handling validation failures, best practices for writing validators,
    and balancing the cost of semantic validation with its benefits. Key points include
    the importance of specific error messages, early validation, focused validator
    responsibilities, and using type hints.
learning/validation/field_level_validation.md:
  hash: a4cf6e4283c5aec2c2ea8b62619650c6
  summary: Field-level validation in Instructor leverages Pydantic to create specific,
    customizable rules for individual data model fields, enabling data validation,
    transformation, and error handling. It includes simple constraints using `Field`,
    complex rules with `field_validator`, and interdependent validations with `model_validator`.
    Best practices emphasize clear messages, data cleaning, and validation order.
    This approach ensures accurate data entry, improves AI prompts, and enhances model
    reliability. Key concepts include field validation, custom validators, validation
    errors, and data quality in AI-driven applications.
learning/validation/retry_mechanisms.md:
  hash: 4c86e7649a26562bebcab4df0f99f0f0
  summary: This guide explains Instructor's retry mechanisms for handling validation
    failures, emphasizing automatic retries with feedback loops that help LLMs generate
    valid responses. It covers how retries work, customizing retry behavior with options
    like max_retries and throw_error, and handling retry failures. The document also
    discusses detailed error messaging, limitations of retries, and advanced patterns
    such as progressive validation with multiple schema layers. Key themes include
    validation, feedback, token consumption, error handling, and customization for
    robust LLM validation workflows.
modes-comparison.md:
  hash: 34ad27dd0581822450f815a8043699ce
  summary: This Mode Comparison Guide explains the different structured data extraction
    modes available in Instructor for various large language model (LLM) providers,
    including OpenAI, Anthropic, Google Gemini, Vertex AI, and more. It highlights
    key modes such as `TOOLS`, `JSON`, `MD_JSON`, and provider-specific options, detailing
    their best use cases, advantages, and compatibility. The guide offers practical
    recommendations for selecting the appropriate mode based on complexity, reliability,
    and provider capabilities, with a focus on optimizing data extraction, structured
    output, and multi-modal inputs. Key keywords include LLM, Instructor modes, AI
    tool calling, JSON output, structured data, OpenAI, Anthropic, Google Gemini,
    Vertex AI, AI prompt engineering, and API integration.
newsletter.md:
  hash: c286128e131ad3635534c9cd9bae2668
  summary: "Subscribe to the Instructor Newsletter to stay updated on AI tips, blog\
    \ posts, research, and new features. The newsletter provides insights into AI\
    \ development, structured outputs, LLM research, and community tricks to enhance\
    \ your projects. Stay informed about Instructor\u2019s latest updates and community\
    \ insights to improve your AI skills and leverage Instructor effectively. Keywords\
    \ include AI updates, Instructor features, structured outputs, LLM research, AI\
    \ development, and community tips."
prompting/decomposition/decomp.md:
  hash: dd1d49ee871acabb8d368a16ea3150fe
  summary: 'Decomposed Prompting leverages a Language Model (LLM) to break down complex
    tasks into manageable sub-tasks, streamlining the problem-solving process. By
    implementing a system of data models and functions, such as `Split`, `StrPos`,
    and `Merge`, this approach enables systematic handling of intricate problems.
    The `derive_action_plan` function orchestrates action plans using specified functions,
    executed step-by-step to achieve the task goals. This modular method optimizes
    LLM performance for challenging tasks, demonstrating effective AI-driven automation
    and problem decomposition. Key terms: Decomposed Prompting, Language Model (LLM),
    task decomposition, AI automation, action plan, modular approach.'
prompting/decomposition/faithful_cot.md:
  hash: f5dd3db43b8242151bac111cab990918
  summary: 'The concept of "Faithful Chain of Thought" in language models focuses
    on enhancing the accuracy of reasoning by dividing the process into two stages:
    Translation and Problem Solving. In the Translation stage, a user query is broken
    down into executable reasoning steps, which are task-specific and deterministically
    executed in the Problem Solving stage, ensuring consistency in the derived answer.
    Examples include converting math word problems into executable Python code, using
    multi-step reasoning in Multi-Hop QA with Python and Datalog, and generating plans
    with symbolic goals through a PDDL Planner. The approach aims to improve the faithfulness
    and effectiveness of language models in problem-solving tasks.'
prompting/decomposition/least_to_most.md:
  hash: 0fda6e18516932d774fbd45486e19af2
  summary: The article discusses the "Least-to-Most" prompting technique for large
    language models (LLMs), which involves breaking down complex problems into a sequence
    of increasingly complex subproblems to facilitate systematic problem-solving.
    It provides an example of this approach using the age problem of Adam and Mary,
    demonstrating how solving simpler subproblems sequentially can aid in answering
    the original complex question. The content includes Python code that outlines
    how to implement this technique using the OpenAI API, including decomposing a
    question into subquestions and solving them one by one. Key ideas include systematic
    decomposition, structured prompting, sequential problem-solving, and application
    to LLMs. The article aims to enhance LLMs' reasoning capabilities by adopting
    this structured methodological approach.
prompting/decomposition/plan_and_solve.md:
  hash: 7efc5f74390a69beeaf130c9b6c31583
  summary: 'Plan and Solve enhances zero-shot Chain of Thought prompting by incorporating
    detailed instructions to improve reasoning accuracy in large language models.
    This approach involves a two-step process: first, devising a comprehensive problem-solving
    plan with explicit reasoning, and second, extracting the final answer based on
    this reasoning. By guiding models to pay closer attention to intermediate calculations
    and logical steps, Plan and Solve achieves more robust performance on various
    reasoning tasks, making it a valuable technique for improving LLM reasoning capabilities
    and accuracy. Key words include zero-shot Chain of Thought, reasoning, prompt
    engineering, large language models, problem-solving, and step-by-step reasoning.'
prompting/decomposition/program_of_thought.md:
  hash: 8413ae10bbc35a4f1128759ca3e4f673
  summary: The "Program Of Thought" is an innovative approach that leverages an external
    Python interpreter to generate intermediate reasoning steps, enhancing performance
    in mathematical and programming tasks. It involves systematically writing executable
    code within designated frameworks, such as the instructor system, to derive precise
    answers. Key features include the use of a specific program prefix, validation
    of code execution, and integration with AI models like GPT-4 to generate detailed
    problem-solving workflows, predictions, and accurate answer selection for complex
    questions. This method aims to ground AI reasoning in deterministic code execution,
    improving accuracy and transparency in problem-solving.
prompting/decomposition/recurs_of_thought.md:
  hash: 5ef001050e89f56ecc769095df6300f4
  summary: This document is a work in progress (WIP) and currently does not contain
    specific content. Once completed, it will outline the core ideas, objectives,
    and key points for effective SEO optimization, focusing on relevant keywords and
    important details.
prompting/decomposition/skeleton_of_thought.md:
  hash: 0aa74871fabd9647ac212ef8198a86b2
  summary: '"Skeleton-of-Thought" is a technique to decrease latency in LLM (Large
    Language Model) pipelines by generating a skeleton outline of a response before
    expanding on each point in parallel. The method involves using parallel API calls
    or batched decoding to enhance efficiency. The core process includes formulating
    a question, creating a brief skeleton outline with 3-10 points, and then expanding
    each point simultaneously. An example implementation with Python demonstrates
    how to achieve this using the `instructor` library and `AsyncOpenAI` for faster
    response generation. Key terms include "Skeleton-of-Thought," "parallel generation,"
    "LLM pipeline," and "response efficiency."'
prompting/decomposition/tree-of-thought.md:
  hash: 5ef001050e89f56ecc769095df6300f4
  summary: The content appears to be a placeholder or work-in-progress (WIP) without
    any available details, title, or description. To optimize for search engines (SEO),
    ensure to include key concepts, objectives, and important keywords once the content
    is finalized. Focus on crafting a summary that highlights central themes or topics,
    such as the purpose of the document, its main points, and any crucial information
    it aims to convey.
prompting/ensembling/cosp.md:
  hash: 4b8eb058102072272fcb938bb8861a5c
  summary: Consistency Based Self Adaptive Prompting (COSP) is an ensemble technique
    designed to enhance large language model (LLM) performance by generating high-quality
    few-shot examples through self-consistency and normalized entropy metrics. It
    automatically selects the most reliable responses from multiple reasoning chains
    based on answer diversity and repetitiveness, then incorporates these examples
    into prompts for improved accuracy. COSP employs strategies like cosine similarity
    for evaluating repetitiveness and aims to optimize answer correctness without
    ground truth labels, making it a key method for self-adaptive prompt engineering,
    ensemble reasoning, and LLM accuracy improvement.
prompting/ensembling/dense.md:
  hash: 4b90091a3795f75f4fc3162a22bf6ec7
  summary: Demonstration Ensembling (DENSE) is a technique to improve language model
    performance by generating multiple responses using different subsets of training
    examples and then aggregating these outputs for a final decision. This method
    involves prompting models like GPT-4 with varied few-shot prompts, partitioning
    examples equally or sampling via embedding clustering. The approach enhances accuracy
    by leveraging self-consistent responses and ensemble methods. Implementation can
    be achieved using tools like the `instructor` library and asynchronous programming
    in Python. Key concepts include few-shot learning, in-context learning, model
    ensembling, prompt engineering, and response aggregation, making DENSE a valuable
    strategy for tasks like classification and decision-making in NLP applications.
prompting/ensembling/diverse.md:
  hash: b93329f06d2f82403fdc0efd37b286f3
  summary: Diverse Verifier On Reasoning Step (DiVeRSe) is an advanced prompting technique
    that enhances reasoning accuracy by generating multiple diverse prompts and leveraging
    AI-based scoring to select the best response. It utilizes self-consistency through
    multiple reasoning paths, combined with a fine-tuned verifier (initially DeBERTa-V3-Large,
    now GPT-4o) to assess response quality and individual reasoning steps. DiVeRSe
    aims to improve multi-step reasoning, accuracy, and robustness in AI models, making
    it suitable for applications like question-answering, problem-solving, and reasoning
    tasks. Key concepts include diverse prompt generation, self-consistency, step-wise
    verification, and AI-based scoring for optimal decision-making in language models.
prompting/ensembling/max_mutual_information.md:
  hash: 2ec748390bb663e6c289e4ec676cb6f2
  summary: Max Mutual Information is a prompting technique for optimizing large language
    models (LLMs) by generating multiple prompt templates and selecting the one that
    maximizes mutual information between the prompt and the model's output. It focuses
    on reducing uncertainty by calculating entropy and mutual information, which measures
    the reduction in entropy when the prompt is used. The method involves estimating
    probabilities and entropies to identify the most effective prompt for eliciting
    accurate responses, especially in complex tasks like story comprehension. Implementation
    involves generating responses with different prompts, scoring model confidence,
    and calculating mutual information to select the best prompt, enhancing LLM performance
    in applications such as the Story Cloze dataset. Key concepts include mutual information,
    entropy, prompt optimization, LLM prompting strategies, and OpenAI API integration.
prompting/ensembling/meta_cot.md:
  hash: d6d91ade7fb984ca99f6e2097c2cb08f
  summary: 'Meta Chain Of Thought (Meta COT) is an advanced reasoning framework that
    decomposes complex queries into multiple sub-questions, aggregates responses,
    and leverages multiple reasoning chains to improve accuracy. Implemented using
    OpenAI''s models, it facilitates step-by-step problem solving by generating sub-queries,
    evaluating reasoning pathways, and synthesizing final answers through a multi-stage
    process. Key features include query decomposition, reasoning chain generation,
    and context-aware final responses, making Meta COT ideal for complex question
    answering, AI reasoning, and improving model accuracy. Keywords: Meta Chain Of
    Thought, multi-step reasoning, query decomposition, AI reasoning, OpenAI, question
    answering, model accuracy.'
prompting/ensembling/more.md:
  hash: 1f26fd2b6a81ae83f6db67299dde096c
  summary: MoRE (Mixture of Reasoning Experts) enhances AI question-answering by combining
    diverse specialized reasoning models, such as Factual, Multihop, Math, and Commonsense
    experts. Each expert employs distinct prompts and reasoning techniques to generate
    responses, which are then scored using a classifier like a random forest to select
    the best answer or abstain if quality is low. A simplified implementation using
    OpenAI's instructor facilitates multi-expert responses and scoring, improving
    overall accuracy across varied reasoning tasks. Key keywords include reasoning
    experts, AI, question answering, multi-step reasoning, factual retrieval, mathematical
    reasoning, commonsense, prompt engineering, and model scoring.
prompting/ensembling/prompt_paraphrasing.md:
  hash: e8f28524643be6affb1b760f6e930184
  summary: 'This guide explores using Large Language Models (LLMs) for back translation
    to enhance prompt performance and diversity. It details methods for paraphrasing
    prompts through translation into different languages and back to English, leveraging
    tools like the instructor package with OpenAI''s GPT-4. The approach improves
    prompt phrasing and robustness, especially for tasks like sentiment analysis of
    user reviews. Key techniques include multilingual translation, prompt variation,
    and leveraging AI for more effective, diverse prompt generation to optimize LLM
    responses. Keywords: Large Language Models, back translation, prompt paraphrasing,
    prompt engineering, multilingual translation, AI prompt optimization, sentiment
    analysis.'
prompting/ensembling/self_consistency.md:
  hash: 6b158b0f8d82d71ae624d4f277ef6824
  summary: Self-Consistency is a technique aimed at improving large language model
    (LLM) performance by generating multiple potential responses and selecting the
    most common answer through majority voting. It involves sampling several candidate
    solutions in parallel and analyzing their consistency to enhance accuracy in tasks
    like question answering. The approach is implemented using Python code with the
    `instructor` library and OpenAI's API, showcasing how to generate and aggregate
    multiple responses to derive the most probable correct answer. This method leverages
    concepts from the research paper "Self-Consistency Improves Chain Of Thought Reasoning
    In Language Models" and emphasizes improved reasoning, accuracy, and model performance
    through sampling, majority voting, and ensemble techniques. Key keywords include
    Self-Consistency, large language models, multiple responses, accuracy, ensemble
    method, majority vote, and chain-of-thought reasoning.
prompting/ensembling/universal_self_consistency.md:
  hash: c56d66bc14be41f9caa4b7b50a9354cb
  summary: Universal Self-Consistency is an advanced approach that enhances traditional
    self-consistency techniques by employing a second large language model (LLM) to
    evaluate and select the most consistent answer among multiple candidates. This
    method improves response diversity and accuracy by supporting various response
    formats and leveraging consensus-based evaluation. Implemented using tools like
    OpenAI's GPT models and the Instructor framework, it involves generating multiple
    responses, assessing their consistency, and choosing the most reliable answer.
    Key concepts include large language models, self-consistency, response evaluation,
    answer selection, and AI accuracy enhancement, making it a valuable strategy for
    improving LLM performance in complex reasoning tasks.
prompting/ensembling/usp.md:
  hash: 3a3df5b548bd422f3f7f84ef8e488300
  summary: "Universal Self Prompting (USP) is a two-step technique for enhancing large\
    \ language models by generating and selecting exemplars from unlabeled data. The\
    \ process involves first creating candidate responses for different task types\u2014\
    classification, short form generation, and long form generation\u2014using specific\
    \ evaluation metrics tailored to each task. These metrics include normalized entropy,\
    \ pairwise ROUGE scores, and label probabilities. In the second step, the best\
    \ examples are appended as prompts for the LLM to produce final predictions with\
    \ a single inference. USP aims to improve model performance across diverse NLP\
    \ tasks through data-driven exemplar generation and selection, utilizing methods\
    \ like confidence-based sampling and task-specific scoring. Keywords include self\
    \ prompting, large language models, unlabeled data, exemplar generation, task-specific\
    \ evaluation, NLP, classification, text summarization, question answering, and\
    \ prompt optimization."
prompting/few_shot/cosp.md:
  hash: c7e5e6103a5c6b02a7c30633495c3282
  summary: 'Consistency Based Self Adaptive Prompting (COSP) is an advanced technique
    for enhancing few-shot learning by selecting high-quality examples based on response
    consistency and confidence metrics such as entropy and repetitiveness. The method
    involves generating multiple responses for potential examples, calculating their
    entropy to measure variability, and evaluating repetitiveness to ensure reliability.
    COSP automates the selection of optimal examples, improving prompt effectiveness
    and model performance, while reducing manual curation. Key features include automated
    example selection, quantifiable quality metrics, and improved accuracy in few-shot
    prompting. Limitations include increased computational cost due to multiple API
    calls, but overall, COSP advances prompt engineering with a focus on consistency
    and confidence metrics for better language model outputs. Keywords: COSP, self-adaptive
    prompting, few-shot learning, response consistency, entropy, repetitiveness, prompt
    optimization, machine learning, language models.'
prompting/few_shot/example_generation/sg_icl.md:
  hash: 68c7f1b6ec1060da68f0da9a83eea8e1
  summary: Self-Generated In-Context Learning (SG-ICL) is a technique that leverages
    large language models (LLMs) to automatically generate example prompts for tasks
    like sentiment analysis. By using tools such as the `instructor` library, SG-ICL
    creates in-context examples that improve model understanding and performance without
    manual data labeling. The method involves generating multiple example reviews
    with associated sentiments, which are then used to guide the model's predictions.
    This approach enhances prompt-based learning, utilizing GPT models like GPT-4,
    and is grounded in recent research on demonstration generation and prompt engineering.
    Key keywords include in-context learning, self-generated examples, LLM, prompt
    engineering, sentiment analysis, GPT, OpenAI, and demonstration generation.
prompting/few_shot/example_ordering.md:
  hash: 46fe78ea46e5f89593be648f251c8628
  summary: This document highlights the significant impact of example ordering in
    few-shot prompting for large language models (LLMs), referencing studies that
    demonstrate how permutating example sequences can improve model performance. It
    discusses various methods to optimize example selection, including manual combinatorics,
    KATE (k-Nearest Example Tuning), and using unsupervised retrieval techniques to
    identify the most relevant in-context examples. These strategies aim to enhance
    few-shot learning, prompt engineering, and prompt relevance, making it essential
    for AI researchers and practitioners to consider example order and selection methods
    to maximize LLM effectiveness. Key keywords include few-shot prompting, LLM, prompt
    optimization, example ordering, KATE, unsupervised retrieval, prompt engineering,
    and in-context learning.
prompting/few_shot/exemplar_selection/knn.md:
  hash: 043cf2bc9050b9d8ac79ce9f24180ca2
  summary: This guide demonstrates how to select effective in-context examples for
    language models using KNN and embeddings. The process involves embedding query
    examples, calculating cosine similarity-based distances, and retrieving the k
    most similar examples to improve response accuracy. The code showcases embedding
    questions, computing distances, selecting closest examples, and generating concise,
    precise answers using OpenAI's GPT-4 model. Keywords include KNN, in-context learning,
    embeddings, cosine similarity, prompt optimization, GPT-4, and language model
    tuning.
prompting/few_shot/exemplar_selection/vote_k.md:
  hash: 5ef001050e89f56ecc769095df6300f4
  summary: The content appears to be a work in progress (wip) and does not include
    specific details or key points yet. To create an effective SEO summary, more information
    about the topic, objectives, and main ideas are needed. Once provided, I can generate
    a concise and keyword-rich summary suitable for SEO purposes.
prompting/index.md:
  hash: f891a11d8cbbc44db1a2d482b361f4fc
  summary: This comprehensive guide explores advanced prompting techniques to enhance
    Large Language Model (LLM) performance, including zero-shot, few-shot, reasoning,
    verification, decomposition, ensembling, and self-criticism methods. It offers
    research-backed strategies for improving accuracy, handling complex problems,
    generating creative content, and verifying factual correctness. The guide also
    details how to implement these techniques using Instructor with Pydantic models,
    providing practical code examples. Keywords include prompt engineering, LLM prompting
    techniques, zero-shot, few-shot, chain of thought, self-verification, decomposition,
    ensembling, and Instructor integration.
prompting/self_criticism/chain_of_verification.md:
  hash: 73ebc5e56042b7f72031c9b68be3dc97
  summary: Chain Of Verification (CoVe) is a method designed to enhance the reliability
    of large language model (LLM) responses through a multi-step validation process.
    It involves generating an initial answer, creating follow-up questions to verify
    key facts and assumptions, independently answering these questions, and finally
    using a final API call to confirm or correct the original response. This approach
    reduces hallucinations and improves accuracy, making it highly effective for ensuring
    trustworthy AI-generated content. Core keywords include LLM verification, AI validation,
    reducing hallucinations, prompt engineering, and response accuracy.
prompting/self_criticism/cumulative_reason.md:
  hash: dc7fbab50e534f394dab15dc2d13816c
  summary: "Cumulative Reasoning enhances large language model performance by dividing\
    \ the reasoning process into three steps: propose, verify, and report. This structured\
    \ approach improves logical inference and mathematical problem-solving accuracy\
    \ by generating potential reasoning steps, validating their correctness, and determining\
    \ the conclusion. Implemented using OpenAI\u2019s API, this method ensures disciplined,\
    \ step-by-step deduction rooted in First-Order Logic, making it ideal for logical,\
    \ mathematical, and AI reasoning tasks. Key concepts include reasoning steps,\
    \ validation, logical inference, and advanced LLM prompting techniques for improved\
    \ reasoning accuracy."
prompting/self_criticism/reversecot.md:
  hash: 718094a1f90e542c567a278e52e4b731
  summary: Reverse Chain Of Thought (RCoT) is a method for identifying logical inconsistencies
    in a large language model's reasoning process by reconstructing the original question
    from the generated solution. This three-step approach involves reconstructing
    the question, pinpointing discrepancies between original and reconstructed conditions,
    and providing targeted feedback for improvement. Implemented via a specialized
    framework, RCoT enhances prompt accuracy, logical coherence, and response quality,
    making it an effective tool for refining AI-generated reasoning and solutions.
    Key concepts include problem reconstruction, inconsistency detection, targeted
    feedback, and improving AI reasoning accuracy.
prompting/self_criticism/self_calibration.md:
  hash: 10cd8050ef8c5a0154316edb507747c1
  summary: Self Calibration is a technique to help language models assess the confidence
    and validity of their responses. By evaluating their output using a structured
    prompt template and tools like the Instructor library, models can generate reasoning
    and determine whether answers are correct, without relying on internal hidden
    states. This approach enhances model reliability by enabling self-assessment of
    knowledge and uncertainties, which is essential for improving question-answering
    accuracy and trustworthiness in AI systems. Key concepts include self-calibration,
    confidence estimation, language model evaluation, prompt engineering, and AI reliability.
prompting/self_criticism/self_refine.md:
  hash: 26954d5fa1eef958210dfcd38b6f1c8a
  summary: 'This article explores the Self-Refine approach to enhance LLM-generated
    responses through iterative feedback and refinement. It details how an LLM can
    generate an initial output, receive feedback on improvements for speed, readability,
    and code quality, and then refine the response accordingly until a stopping condition
    is met. The process repeats using the same model, improving outputs step-by-step.
    Key concepts include iterative self-improvement, feedback loops, LLM refinement,
    and Python coding examples, with references to research on self-feedback techniques
    and prompting strategies. Keywords: Self-Refine, feedback loop, LLM, iterative
    refinement, prompt engineering, code optimization, natural language processing.'
prompting/self_criticism/self_verification.md:
  hash: 77d9f2d4e8bf08216987b11d2bf8679a
  summary: 'This document outlines a self-verification framework for validating Large
    Language Model (LLM) responses through a two-stage process: forward reasoning
    and backward verification. The approach involves generating multiple response
    candidates using chain-of-thought reasoning, then verifying each candidate by
    rewriting the question into a declarative form and constructing verification prompts
    using True-False Item Verification (TFV) or Condition Mask Verification (CMV).
    The verification process repeats multiple times, and the candidate with the highest
    verification score is selected as the final answer. The framework is implemented
    with code examples using OpenAI''s API and aims to improve the accuracy and reliability
    of LLM outputs. Key concepts include self-verification, prompt engineering, declarative
    rewriting, LLM verification, chain-of-thought, and model prompting techniques.'
prompting/thought_generation/chain_of_thought_few_shot/active_prompt.md:
  hash: ec50ae930bfa92be2db89c937e696404
  summary: 'Active prompting is a technique to enhance Large Language Model (LLM)
    performance by selecting effective examples for human annotation. This process
    involves four main steps: uncertainty estimation, selection, annotation, and inference.
    The uncertainty estimation step uses metrics like disagreement, entropy, and variance
    to measure how confident the LLM is in its responses. By querying the LLM multiple
    times, the differences in responses indicate areas of uncertainty. Selection involves
    choosing the most uncertain examples for human annotation, which are then used
    to improve the LLM''s inference capabilities. This method optimizes the use of
    labeled data to boost LLM accuracy and performance.'
prompting/thought_generation/chain_of_thought_few_shot/auto_cot.md:
  hash: aa45163a89881ec54d814f68e369d2df
  summary: The article discusses improving the performance of few-shot Chain of Thought
    (CoT) reasoning by automating the selection of diverse examples. The method involves
    clustering potential examples, sorting them based on distance from cluster centers,
    and selecting those that meet predefined criteria, such as a maximum of five reasoning
    steps. This automated approach reduces reasoning errors by ensuring the examples
    are varied and representative. The implementation includes clustering with KMeans,
    encoding with Sentence Transformers, and using AI models like GPT-4 for processing.
    This technique enhances large language models' accuracy by systematically selecting
    examples for optimal performance. Key terms include few-shot CoT, clustering,
    diverse examples, reasoning error reduction, and automated example selection.
prompting/thought_generation/chain_of_thought_few_shot/complexity_based.md:
  hash: 08f5ce3a728a741234799bbaaede1acf
  summary: 'The article discusses "Complexity Based Prompting" to enhance language
    model performance by selecting examples with more reasoning steps or longer responses
    when reasoning lengths aren''t available. This approach, known as "Complexity
    Based Consistency," involves sampling multiple responses and selecting the most
    complex ones based on reasoning step length. The process is implemented using
    tools like `instructor` and `AsyncOpenAI`, leveraging structured reasoning steps
    in query responses. By generating and ranking multiple responses, the method identifies
    top responses to derive accurate answers, as demonstrated with a practical example.
    Keywords: Complexity Based Prompting, language models, multi-step reasoning, AI
    performance, Complexity Based Consistency, `instructor`, `AsyncOpenAI`.'
prompting/thought_generation/chain_of_thought_few_shot/contrastive.md:
  hash: 607e1e5586ac745bccb961f0df089c17
  summary: The document discusses the technique of Contrastive Chain Of Thought (CoT)
    to enhance language model performance by deliberately including incorrect reasoning
    examples alongside correct ones during training. This method helps the AI learn
    from mistakes and improve its response generation. The approach involves using
    a specific template with correct and incorrect examples to guide the AI in providing
    accurate answers. An example implementation is provided using Python and the `instructor`
    package to demonstrate the process. Key concepts include chain-of-thought prompting,
    incorrect reasoning, language model training, and AI performance enhancement.
prompting/thought_generation/chain_of_thought_few_shot/memory_of_thought.md:
  hash: 5ef001050e89f56ecc769095df6300f4
  summary: It seems like the content is still a work in progress, as indicated by
    the "[wip]" tag. Since the title, description, and keywords are left empty, more
    information is needed to provide an accurate SEO summary. To optimize for SEO,
    consider focusing on the main topic of the content, its objectives, and any unique
    selling points or important details. Once more details are available, including
    keywords relevant to the content's subject, an effective summary can be crafted
    to improve search visibility.
prompting/thought_generation/chain_of_thought_few_shot/prompt_mining.md:
  hash: 214b95070291158fec9b154f77370f57
  summary: 'The article discusses "Prompt Mining," a technique used to enhance the
    performance of Large Language Models (LLMs) by discovering effective prompt formats
    from text corpora, such as Wikipedia. The approach aims to identify better prompt
    structures that allow LLMs to respond more accurately. It contrasts manual prompts
    with mined prompts, presenting examples of both to illustrate improved prompt
    efficiency. The document outlines a method using the `instructor` library, demonstrating
    how to implement Prompt Mining to generate concise and clear prompt templates.
    Key points include the importance of prompt formatting, the use of placeholder
    templates, and the effectiveness of automated prompt discovery in improving language
    model outputs. Keywords: Prompt Mining, Large Language Models, prompt templates,
    language model performance, automated prompt discovery, `instructor` library.'
prompting/thought_generation/chain_of_thought_few_shot/uncertainty_routed_cot.md:
  hash: b90fa988c085d0dde6594aa75eac0544
  summary: "The Uncertainty-Routed Chain Of Thought technique, detailed in the Gemini\
    \ Paper, enhances traditional Chain Of Thought methods by generating multiple\
    \ reasoning chains\u2014either 8 or 32\u2014and selecting the majority answer\
    \ only if it meets a specified threshold of agreement. Implemented in Python with\
    \ OpenAI's models, this approach involves using asynchronous prompts to create\
    \ a batch of responses, counting the majority vote, and comparing it to the confidence\
    \ threshold (e.g., 0.6) to determine the final answer. This technique is designed\
    \ to improve the accuracy and reliability of AI-generated answers in complex decision-making\
    \ scenarios. Key elements include uncertainty routing, batch processing, majority\
    \ voting, and threshold evaluation."
prompting/thought_generation/chain_of_thought_zero_shot/analogical_prompting.md:
  hash: daa15bd030a6f2d0584e310e29f781c0
  summary: 'Analogical Prompting is a method designed to enhance the accuracy of large
    language models (LLMs) by prompting the model to generate relevant examples before
    addressing a user''s query. This technique leverages the extensive knowledge acquired
    by the LLM during training, encouraging it to recall pertinent problems and solutions.
    The process involves providing a problem, recalling three relevant and distinct
    problems with their solutions, and then solving the initial problem. A Python
    implementation using the `instructor` module demonstrates this method with an
    example query about calculating the area of a square using given vertices. This
    approach is based on research into LLMs as analogical reasoners, aimed at improving
    problem-solving capabilities. Key points include the use of templates, structured
    recall of problem-solving instances, and enhanced accuracy in query responses.
    Keywords: Analogical Prompting, large language models, LLMs, problem-solving,
    language model training, accuracy enhancement, Python implementation, example
    generation, query response.'
prompting/thought_generation/chain_of_thought_zero_shot/step_back_prompting.md:
  hash: 266f50f0729c9faf17ee37f0ee9ef6a2
  summary: Step-back prompting is a two-step technique utilized with Large Language
    Models (LLMs) to improve contextual understanding and reasoning capabilities.
    The method involves first asking a high-level, topic-specific question, known
    as the "step-back question," to gather broader context. This is followed by "abstracted-grounded
    reasoning," where the LLM answers the initial query within the context provided
    by the step-back response. This technique has proven effective in enhancing performance
    on reasoning benchmarks for models like PaLM-2L and GPT-4. The implementation
    often involves generating step-back questions with LLM queries to ensure precise
    abstract questioning.
prompting/thought_generation/chain_of_thought_zero_shot/tab_cot.md:
  hash: 9d53b891d95c8c14d3bd15758757e736
  summary: 'The text discusses the concept of Tabular Chain of Thought (Tab-CoT),
    a method to improve the reasoning and output quality of language models by structuring
    their reasoning in the form of markdown tables. It introduces a process using
    Python, OpenAI, and the `instructor` library to generate structured reasoning
    responses. This approach involves defining reasoning steps as objects, breaking
    down queries into subquestions, and detailing procedures and results, thus enhancing
    clarity and precision in model outputs. The example provided calculates the remaining
    loaves of bread at a bakery, showcasing the structured reasoning process. Keywords:
    Tabular Chain of Thought, Tab-CoT, language models, structured reasoning, markdown
    tables, Python, OpenAI, reasoning steps.'
prompting/thought_generation/chain_of_thought_zero_shot/thread_of_thought.md:
  hash: 2549f9996ba2068ab4cfd1b7f23cb083
  summary: The article introduces the "Thread of Thought" technique, which enhances
    AI model responses by systematically focusing on relevant context and ignoring
    irrelevant information. This method improves reasoning performance and response
    quality by encouraging models to analyze and summarize information incrementally.
    The implementation involves using templates in Python with the OpenAI API to assess
    each piece of context for its significance. Key phrases and approaches are suggested
    for guiding models through the context effectively. This technique can be particularly
    useful for complex question-answering tasks that involve large datasets or lengthy
    documents.
prompting/zero_shot/emotion_prompting.md:
  hash: a9ad30ffe419f260e612691bf23edf9f
  summary: This article explores the use of emotional stimuli in prompts to enhance
    the performance of language models. It highlights how adding emotionally significant
    phrases, such as "This is very important to my career," can influence model responses.
    The implementation example demonstrates prompting GPT-4 with emotional cues to
    generate curated outputs, like a list of musical albums from the 2000s. The content
    references research on emotional stimuli's impact on large language models and
    provides code snippets for practical application. Keywords include emotion prompting,
    language models, emotional stimuli, prompt engineering, GPT-4, AI performance,
    and AI enhancement.
prompting/zero_shot/rar.md:
  hash: 805a3c367401933b2ff8f5d4e29a65ea
  summary: "This guide explains how to improve AI understanding of ambiguous prompts\
    \ by using rephrase-and-respond (RaR) techniques. It demonstrates how to identify\
    \ and clarify vague questions\u2014such as interpreting \"odd month\"\u2014by\
    \ prompting models to rephrase and expand queries for better accuracy. The implementation\
    \ involves a two-step process: first rephrasing the question, then generating\
    \ the final response, leveraging examples and code snippets based on a research\
    \ paper. Key concepts include prompt clarification, ambiguity resolution, AI prompt\
    \ engineering, and enhancing natural language understanding with GPT models. Keywords:\
    \ ambiguity resolution, rephrasing, prompt clarification, GPT, AI response improvement,\
    \ RaR method, prompt engineering."
prompting/zero_shot/re2.md:
  hash: 47fa785b3078b3e12127b6a5c135dcc7
  summary: Re2 (Re-Reading) is a prompting technique designed to enhance a language
    model's understanding and reasoning by having it read the question again. This
    method involves prompting the model to re-examine the query with critical thinking
    cues, such as "Let's think step by step," to improve accuracy and reasoning in
    responses. An implementation example using the GPT-4 model demonstrates how re-reading
    prompts can guide the model to arrive at correct answers. Key concepts include
    prompt engineering, critical thinking prompts, and improving reasoning in large
    language models, supported by research from the paper "Re-Reading Improves Reasoning
    in Large Language Models."
prompting/zero_shot/role_prompting.md:
  hash: d42a120d6c34f85fd648775d4a6c88be
  summary: Role prompting, also known as persona prompting, enhances large language
    model performance on open-ended tasks by assigning specific or social roles to
    the AI. This approach involves framing the model with roles such as "a talented
    writer" or "a helpful AI assistant" to guide its responses. Implementation includes
    creating system prompts with role descriptions, as demonstrated through Python
    code using OpenAI and instructor libraries, with examples like generating a poem
    about coffee. Key concepts include systematic role selection, multi-persona collaboration,
    and references to relevant research papers like RoleLLM. This technique is essential
    for improving AI flexibility and effectiveness in various conversational and creative
    tasks.
prompting/zero_shot/s2a.md:
  hash: f3b55fc1bf5a617fa1dd82134ecaa495
  summary: 'The System 2 Attention (S2A) technique enhances prompt relevance by auto-refining
    user input through a two-step process: rewriting prompts to include only pertinent
    information and then generating accurate responses. Implemented using GPT-4, S2A
    leverages prompt engineering inspired by recent research (arXiv:2311.11829) to
    improve model focus and answer precision. Key features include extracting relevant
    context from user queries and minimizing irrelevant data, making it valuable for
    optimized AI communication, prompt refinement, and advanced language model applications.
    Keywords: System 2 Attention, prompt refinement, AI prompt engineering, GPT-4,
    relevance extraction, model focus, arXiv 2311.11829.'
prompting/zero_shot/self_ask.md:
  hash: f25cf054eea8c90dcca3ab21a56f51b7
  summary: Self-Ask is an innovative prompting technique designed to improve language
    model reasoning by addressing the compositionality gap. It encourages models to
    determine if follow-up questions are needed, generate and answer those questions,
    and then use these answers to produce a more accurate overall solution. Implemented
    using a zero-shot prompt with the instructor framework, Self-Ask enhances the
    ability of models like GPT-4 to handle complex queries through dynamic sub-problem
    solving. Key concepts include compositionality gap, follow-up questions, zero-shot
    prompting, and sub-problem answering for improved reasoning accuracy.
prompting/zero_shot/simtom.md:
  hash: b6f1003c8f869a54c705cd1f71861c44
  summary: SimToM (Simulated Theory of Mind) is a two-step prompting technique designed
    to enhance large language models' ability to consider specific perspectives. It
    involves first isolating relevant information related to an entity within a context,
    and then asking the model to answer questions solely based on those facts from
    the entity's viewpoint. This method is especially useful for complex scenarios
    with multiple entities, improving the model's understanding and reasoning about
    different perspectives. Implementation includes structured prompts and code examples
    using OpenAI's GPT-4, focusing on perspective-taking and context-specific responses.
    Key concepts include perspective-taking, multi-entity reasoning, and advanced
    prompt engineering for improved model comprehension.
prompting/zero_shot/style_prompting.md:
  hash: 75dcd0e32542292f3fcf222298633967
  summary: This article explains how to constrain large language model outputs through
    prompt engineering by specifying stylistic parameters such as writing style, tone,
    mood, and genre. It demonstrates implementing stylistic constraints using prompts
    and provides examples, including generating emails with different tones like formal
    or informal. The content emphasizes that prompt-based style directives can effectively
    tailor responses, with key keywords including prompt engineering, stylistic constraints,
    language model customization, tone, genre, and AI text generation.
repository-overview.md:
  hash: 16a893aa592a4478f0bd70ce059ce714
  summary: The Instructor repository provides a comprehensive codebase for structured
    output management, featuring core libraries in the `instructor/` directory, and
    command-line tools in `cli/`. It also includes documentation sources in `docs/`,
    practical examples in `examples/`, and testing scripts in `tests/`. This layout
    supports efficient development, usage, and evaluation of Instructor's functionalities
    for clients, adapters, utilities, and job management, making it essential for
    developers working on structured output tasks.
start-here.md:
  hash: c2988edc908b8bc2e27bb0dde43da4cd
  summary: Instructor is a beginner-friendly Python library designed to extract structured,
    predictable data from large language models like GPT-4 and Claude. It simplifies
    working with LLMs by allowing users to define data response models using Pydantic,
    ensuring accurate formatting and validation of outputs. Key features include response
    models, mode selection (such as JSON and tools), and seamless integration with
    providers like OpenAI. Instructor is ideal for use cases like data extraction,
    form filling, classification, content generation, and API integration, making
    structured output from language models accessible and reliable for developers.
templates/concept_template.md:
  hash: 633cb40e0ffaa2d67a189854c0504928
  summary: 'The document provides a comprehensive guide on using the [Concept Name]
    within the Instructor platform, emphasizing its importance and practical utility.
    It includes a detailed overview of the concept, scenarios where it is beneficial,
    and instructions for basic and advanced usage with Python code examples. The guide
    covers integration with various LLM providers like OpenAI and Anthropic and emphasizes
    best practices and common patterns. It also includes links to related concepts
    and complete examples, making it a valuable resource for understanding and implementing
    [Concept Name] effectively. Keywords: [Concept Name], Instructor, integration,
    LLM providers, Python, OpenAI, Anthropic, best practices, usage examples.'
templates/cookbook_template.md:
  hash: 41e7d7b42733c153be1b60aa64eb6ec3
  summary: "This comprehensive tutorial demonstrates how to harness Instructor with\
    \ OpenAI's language models to extract structured data from text. It covers installation,\
    \ prerequisites, and step-by-step implementation, including defining pydantic\
    \ models, configuring the OpenAI client with explicit JSON mode, and processing\
    \ input data for reliable extraction. Key concepts include using instructor\u2019\
    s mode, model versioning, exception handling, and customization options for tailored\
    \ data extraction. This guide is essential for developers aiming to automate data\
    \ parsing and improve AI-driven information retrieval."
templates/provider_template.md:
  hash: 8d45c11fd17a15b511a5d0ba1a66538d
  summary: This comprehensive guide details how to use [Provider Name] with the Instructor
    library, offering both quick start and advanced examples for extracting structured
    data from text. It covers setup instructions, environment configuration, and provides
    code snippets for synchronous and asynchronous use cases. Key features include
    support for multiple instructor modes, streaming results, and provider-specific
    options. The documentation highlights the process of authenticating via API keys,
    defining data models with Pydantic, and selecting appropriate models for optimal
    results, making it a valuable resource for developers integrating [Provider Name]'s
    AI capabilities for data extraction and natural language understanding.
tutorials/index.md:
  hash: 5fede8f11885b727e75019feb254b386
  summary: The Instructor Tutorials page offers a comprehensive, step-by-step learning
    pathway for mastering AI and large language models (LLMs) using the Instructor
    framework. It features interactive Jupyter notebooks accessible via local setup,
    Google Colab, or Binder, covering topics from basic extraction and prompts to
    advanced applications like Retrieval-Augmented Generation (RAG), validation, knowledge
    graphs, and synthetic data creation. The tutorials aim to build practical skills
    in structured data extraction, validation, streaming responses, and multi-provider
    support. Key concepts include LLM optimization, production techniques, and real-world
    AI applications, supported by resources like documentation, community forums,
    and example projects.
why.md:
  hash: 689c8981bbf25ac89b583505ca8e73d4
  summary: Instructor is a tool that simplifies working with large language models
    (LLMs) by providing structured, reliable outputs without the need for complex
    parsing or provider-specific code. It offers automatic validation and retries,
    supporting complex data schemas, nested objects, and lists through easy-to-use
    Python models. Instructor reduces development time, minimizes bugs, and ensures
    consistent results across multiple LLM providers like OpenAI, Anthropic, and Google
    Gemini. Ideal for building robust, production-ready AI applications, it eliminates
    the common challenges of unstructured outputs and provider variability.
